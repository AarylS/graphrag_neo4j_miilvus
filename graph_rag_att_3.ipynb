{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "set_debug(False)\n",
    "set_verbose(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "\n",
    "llm_name = \"gemini-2.0-flash-lite\"     # answer llm\n",
    "graph_llm_name = \"gemini-2.5-flash-lite\" \n",
    "embed_model_name = \"all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fuths/projects/graphrag_2/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# embedding model\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# model_name=\"all-MiniLM-L6-v2\"\n",
    "# model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [does answer generation, rest of the work] llm - huggingface model\n",
    "\n",
    "# from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=repo_id,\n",
    "#     max_new_tokens=1024,\n",
    "#     do_sample=False,\n",
    "#     temperature=0,\n",
    "#     provider=\"auto\",  # let Hugging Face choose the best provider for you\n",
    "# )\n",
    "\n",
    "# chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# \"\"\" usage example\n",
    "\n",
    "# messages = \"tell a joke\"\n",
    "# ai_msg = chat_model.invoke(messages)\n",
    "\n",
    "# print(ai_msg.content)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph llm - cypher generation\n",
    "\n",
    "# google gemini\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "graph_llm = ChatGoogleGenerativeAI(model=graph_llm_name, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_672463/355719928.py:5: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"qwen3:4b\", format=\"json\", temperature=0)\n"
     ]
    }
   ],
   "source": [
    "# answering llm\n",
    "\n",
    "# gemini\n",
    "llm = ChatGoogleGenerativeAI(model=llm_name, temperature=0)\n",
    "\n",
    "# ollama qwen3 4b\n",
    "# from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# llm = ChatOllama(model=\"qwen3:4b\", format=\"json\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDFs from: ./data_stored/\n",
      "  - Loading document: Essential-GraphRAG.pdf\n",
      "  - Loading document: OReilly_Graph_Databases.pdf\n",
      "Loaded a total of 4 pages from 2 PDF files.\n"
     ]
    }
   ],
   "source": [
    "# get and load documents\n",
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "pdf_folder_path = \"./data_stored/\"\n",
    "print(f\"Loading PDFs from: {pdf_folder_path}\")\n",
    "\n",
    "pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith(\".pdf\")]\n",
    "\n",
    "all_documents = []\n",
    "for pdf_file in pdf_files:\n",
    "    file_path = os.path.join(pdf_folder_path, pdf_file)\n",
    "    print(f\"  - Loading document: {pdf_file}\")\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "\n",
    "    \"\"\" uncomment the lines below for the demo example \n",
    "    # start_index = 55  \n",
    "    # end_index = 57   \n",
    "\n",
    "    # cleaned_pages = [\n",
    "    #     page for page in pages \n",
    "    #     if start_index <= page.metadata.get('page', 0) < end_index\n",
    "    # ]\n",
    "\n",
    "    # all_documents.extend(cleaned_pages)\n",
    "    \"\"\"\n",
    "\n",
    "    # comment the line below for the demo example\n",
    "    all_documents.extend(pages)\n",
    "\n",
    "print(f\"Loaded a total of {len(all_documents)} pages from {len(pdf_files)} PDF files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the documents into 6 chunks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the loaded documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"Split the documents into {len(doc_splits)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fuths/projects/graphrag_2/.venv/lib/python3.11/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    }
   ],
   "source": [
    "# creating vectordb\n",
    "\n",
    "from langchain_milvus import Milvus\n",
    "\n",
    "# Add to Milvus\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag_milvus_att_5\",\n",
    "    embedding=embedding_model,\n",
    "    connection_args={\"uri\": \"./milvus_ingest_att_5.db\"},\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_672463/4244973288.py:12: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
      "  graph = Neo4jGraph()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "end of chunk, time taken: 16.40524172782898\n",
      "end of chunk, time taken: 30.17276668548584\n",
      "end of chunk, time taken: 23.614866733551025\n",
      "end of chunk, time taken: 40.06956386566162\n",
      "end of chunk, time taken: 32.98189043998718\n",
      "end of chunk, time taken: 22.406184673309326\n",
      "2\n",
      "3\n",
      "Graph documents: 6\n",
      "Nodes from 1st graph doc:[Node(id='Step-Back Prompting', type='Technique', properties={}), Node(id='Llm', type='Model', properties={}), Node(id='Query Rewriting Task', type='Task', properties={}), Node(id='Natural Language Comprehension', type='Capability', properties={}), Node(id='Natural Language Generation', type='Capability', properties={}), Node(id='Authors Of The Step-Back Prompting Paper', type='Group', properties={}), Node(id='System Prompt', type='Instruction', properties={}), Node(id='Zero-Shot Prompting', type='Technique', properties={}), Node(id='Thierry Audel', type='Person', properties={}), Node(id='Few-Shot Examples', type='Concept', properties={}), Node(id='General Capabilities', type='Capability', properties={})]\n",
      "Relationships from 1st graph doc:[Relationship(source=Node(id='Step-Back Prompting', type='Technique', properties={}), target=Node(id='Llm', type='Model', properties={}), type='USES', properties={}), Relationship(source=Node(id='Llm', type='Model', properties={}), target=Node(id='Query Rewriting Task', type='Task', properties={}), type='PERFORMS', properties={}), Relationship(source=Node(id='Llm', type='Model', properties={}), target=Node(id='Natural Language Comprehension', type='Capability', properties={}), type='EXCELS_AT', properties={}), Relationship(source=Node(id='Llm', type='Model', properties={}), target=Node(id='Natural Language Generation', type='Capability', properties={}), type='EXCELS_AT', properties={}), Relationship(source=Node(id='Authors Of The Step-Back Prompting Paper', type='Group', properties={}), target=Node(id='System Prompt', type='Instruction', properties={}), type='USED', properties={}), Relationship(source=Node(id='System Prompt', type='Instruction', properties={}), target=Node(id='Llm', type='Model', properties={}), type='INSTRUCTS', properties={}), Relationship(source=Node(id='System Prompt', type='Instruction', properties={}), target=Node(id='Few-Shot Examples', type='Concept', properties={}), type='CONTAINS', properties={}), Relationship(source=Node(id='Zero-Shot Prompting', type='Technique', properties={}), target=Node(id='Llm', type='Model', properties={}), type='RELY_ON', properties={}), Relationship(source=Node(id='Llm', type='Model', properties={}), target=Node(id='General Capabilities', type='Capability', properties={}), type='HAS', properties={}), Relationship(source=Node(id='Thierry Audel', type='Person', properties={}), target=Node(id='Few-Shot Examples', type='Concept', properties={}), type='IS_EXAMPLE_IN', properties={})]\n"
     ]
    }
   ],
   "source": [
    "# GraphRAG Setup\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "import time\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "\n",
    "\n",
    "graph_transformer = LLMGraphTransformer(\n",
    "    llm=graph_llm,\n",
    "    # allowed_nodes=[\"Concept\"],\n",
    "    # node_properties=[\"\"],\n",
    "    # allowed_relationships=[\"AUTHORED\", \"\", \"RELATED_TO\"],\n",
    ")\n",
    "print(\"1\")\n",
    "\n",
    "graph_documents = []\n",
    "for i,doc in enumerate(doc_splits):\n",
    "    stime = time.time()\n",
    "    # Process one document at a time\n",
    "    graph_doc = graph_transformer.convert_to_graph_documents([doc])\n",
    "    graph_documents.extend(graph_doc)\n",
    "    print(f\"end of chunk, time taken: {time.time() - stime}\")\n",
    "    \n",
    "print(\"2\")\n",
    "graph.add_graph_documents(graph_documents)\n",
    "print(\"3\")\n",
    "print(f\"Graph documents: {len(graph_documents)}\")\n",
    "print(f\"Nodes from 1st graph doc:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships from 1st graph doc:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0:\n",
      "  Nodes: [Node(id='Step-Back Prompting', type='Technique', properties={}), Node(id='Llm', type='Model', properties={}), Node(id='Query Rewriting Task', type='Task', properties={}), Node(id='Natural Language Comprehension', type='Capability', properties={}), Node(id='Natural Language Generation', type='Capability', properties={}), Node(id='Authors Of The Step-Back Prompting Paper', type='Group', properties={}), Node(id='System Prompt', type='Instruction', properties={}), Node(id='Zero-Shot Prompting', type='Technique', properties={}), Node(id='Thierry Audel', type='Person', properties={}), Node(id='Few-Shot Examples', type='Concept', properties={}), Node(id='General Capabilities', type='Capability', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Step-Back Prompting', type='Technique', properties={}), target=Node(id='Llm', type='Model', properties={}), type='USES', properties={}), Relationship(source=Node(id='Llm', type='Model', properties={}), target=Node(id='Query Rewriting Task', type='Task', properties={}), type='PERFORMS', properties={}), Relationship(source=Node(id='Llm', type='Model', properties={}), target=Node(id='Natural Language Comprehension', type='Capability', properties={}), type='EXCELS_AT', properties={}), Relationship(source=Node(id='Llm', type='Model', properties={}), target=Node(id='Natural Language Generation', type='Capability', properties={}), type='EXCELS_AT', properties={}), Relationship(source=Node(id='Authors Of The Step-Back Prompting Paper', type='Group', properties={}), target=Node(id='System Prompt', type='Instruction', properties={}), type='USED', properties={}), Relationship(source=Node(id='System Prompt', type='Instruction', properties={}), target=Node(id='Llm', type='Model', properties={}), type='INSTRUCTS', properties={}), Relationship(source=Node(id='System Prompt', type='Instruction', properties={}), target=Node(id='Few-Shot Examples', type='Concept', properties={}), type='CONTAINS', properties={}), Relationship(source=Node(id='Zero-Shot Prompting', type='Technique', properties={}), target=Node(id='Llm', type='Model', properties={}), type='RELY_ON', properties={}), Relationship(source=Node(id='Llm', type='Model', properties={}), target=Node(id='General Capabilities', type='Capability', properties={}), type='HAS', properties={}), Relationship(source=Node(id='Thierry Audel', type='Person', properties={}), target=Node(id='Few-Shot Examples', type='Concept', properties={}), type='IS_EXAMPLE_IN', properties={})]\n",
      "---\n",
      "Document 1:\n",
      "  Nodes: [Node(id='Few-Shot Prompting', type='Technique', properties={}), Node(id='Llm', type='Model', properties={}), Node(id='Query Rewriting', type='Technique', properties={}), Node(id='System Prompt', type='Concept', properties={}), Node(id='User Question', type='Concept', properties={}), Node(id='Generate_Stepback', type='Function', properties={}), Node(id='Thierry Audel', type='Person', properties={}), Node(id='Step-Back Question', type='Concept', properties={}), Node(id='Parent Document Retriever', type='Strategy', properties={}), Node(id='Document', type='Document', properties={}), Node(id='Section', type='Concept', properties={}), Node(id='Embedding', type='Concept', properties={}), Node(id='Retrieval Accuracy', type='Concept', properties={}), Node(id='Recall', type='Concept', properties={}), Node(id='Output Quality', type='Concept', properties={}), Node(id='Output Reliability', type='Concept', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Few-Shot Prompting', type='Technique', properties={}), target=Node(id='Llm', type='Model', properties={}), type='GUIDES', properties={}), Relationship(source=Node(id='Few-Shot Prompting', type='Technique', properties={}), target=Node(id='Output Quality', type='Concept', properties={}), type='IMPROVES', properties={}), Relationship(source=Node(id='Few-Shot Prompting', type='Technique', properties={}), target=Node(id='Output Reliability', type='Concept', properties={}), type='IMPROVES', properties={}), Relationship(source=Node(id='Query Rewriting', type='Technique', properties={}), target=Node(id='System Prompt', type='Concept', properties={}), type='UTILIZES', properties={}), Relationship(source=Node(id='Query Rewriting', type='Technique', properties={}), target=Node(id='User Question', type='Concept', properties={}), type='UTILIZES', properties={}), Relationship(source=Node(id='Generate_Stepback', type='Function', properties={}), target=Node(id='Query Rewriting', type='Technique', properties={}), type='PERFORMS', properties={}), Relationship(source=Node(id='Generate_Stepback', type='Function', properties={}), target=Node(id='Step-Back Question', type='Concept', properties={}), type='CREATES', properties={}), Relationship(source=Node(id='Generate_Stepback', type='Function', properties={}), target=Node(id='User Question', type='Concept', properties={}), type='TRANSFORMS', properties={}), Relationship(source=Node(id='Step-Back Question', type='Concept', properties={}), target=Node(id='User Question', type='Concept', properties={}), type='BROADENS_CONTEXT_FOR', properties={}), Relationship(source=Node(id='Generate_Stepback', type='Function', properties={}), target=Node(id='Retrieval Accuracy', type='Concept', properties={}), type='ENHANCES', properties={}), Relationship(source=Node(id='Generate_Stepback', type='Function', properties={}), target=Node(id='Recall', type='Concept', properties={}), type='ENHANCES', properties={}), Relationship(source=Node(id='Parent Document Retriever', type='Strategy', properties={}), target=Node(id='Document', type='Document', properties={}), type='DIVIDES', properties={}), Relationship(source=Node(id='Document', type='Document', properties={}), target=Node(id='Section', type='Concept', properties={}), type='HAS_PART', properties={}), Relationship(source=Node(id='Parent Document Retriever', type='Strategy', properties={}), target=Node(id='Embedding', type='Concept', properties={}), type='COMPUTES', properties={}), Relationship(source=Node(id='Embedding', type='Concept', properties={}), target=Node(id='Section', type='Concept', properties={}), type='ASSOCIATED_WITH', properties={}), Relationship(source=Node(id='Parent Document Retriever', type='Strategy', properties={}), target=Node(id='Embedding', type='Concept', properties={}), type='EMPLOYS', properties={}), Relationship(source=Node(id='Embedding', type='Concept', properties={}), target=Node(id='User Question', type='Concept', properties={}), type='MATCHES', properties={}), Relationship(source=Node(id='User Question', type='Concept', properties={}), target=Node(id='Thierry Audel', type='Person', properties={}), type='ABOUT', properties={}), Relationship(source=Node(id='Step-Back Question', type='Concept', properties={}), target=Node(id='Thierry Audel', type='Person', properties={}), type='ABOUT', properties={})]\n",
      "---\n",
      "Document 2:\n",
      "  Nodes: [Node(id='Parent Document Retriever Strategy', type='Strategy', properties={}), Node(id='Large Document', type='Document', properties={}), Node(id='Smaller Sections', type='Section', properties={}), Node(id='Embeddings', type='Concept', properties={}), Node(id='User Queries', type='Query', properties={}), Node(id='Listing 3.2', type='Listing', properties={}), Node(id='Function To Generate A Step-Back Question', type='Function', properties={}), Node(id='Listing 3.3', type='Listing', properties={}), Node(id='Executing The Step-Back Prompt Function', type='Process', properties={}), Node(id='Exercise 3.1', type='Exercise', properties={}), Node(id='Step-Back Prompt Generation', type='Process', properties={}), Node(id='Various Questions', type='Question', properties={}), Node(id='Context', type='Concept', properties={}), Node(id='System Prompt', type='Concept', properties={}), Node(id='Output', type='Concept', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Parent Document Retriever Strategy', type='Strategy', properties={}), target=Node(id='Large Document', type='Document', properties={}), type='PROCESSES', properties={}), Relationship(source=Node(id='Large Document', type='Document', properties={}), target=Node(id='Smaller Sections', type='Section', properties={}), type='COMPOSED_OF', properties={}), Relationship(source=Node(id='Smaller Sections', type='Section', properties={}), target=Node(id='Embeddings', type='Concept', properties={}), type='HAS', properties={}), Relationship(source=Node(id='Embeddings', type='Concept', properties={}), target=Node(id='User Queries', type='Query', properties={}), type='MATCHES', properties={}), Relationship(source=Node(id='Listing 3.2', type='Listing', properties={}), target=Node(id='Function To Generate A Step-Back Question', type='Function', properties={}), type='REFERENCES', properties={}), Relationship(source=Node(id='Listing 3.3', type='Listing', properties={}), target=Node(id='Executing The Step-Back Prompt Function', type='Process', properties={}), type='REFERENCES', properties={}), Relationship(source=Node(id='Exercise 3.1', type='Exercise', properties={}), target=Node(id='Step-Back Prompt Generation', type='Process', properties={}), type='EXPLORES', properties={}), Relationship(source=Node(id='Step-Back Prompt Generation', type='Process', properties={}), target=Node(id='Various Questions', type='Question', properties={}), type='APPLIES_TO', properties={}), Relationship(source=Node(id='Step-Back Prompt Generation', type='Process', properties={}), target=Node(id='Context', type='Concept', properties={}), type='BROADENS', properties={}), Relationship(source=Node(id='System Prompt', type='Concept', properties={}), target=Node(id='Output', type='Concept', properties={}), type='AFFECTS', properties={})]\n",
      "---\n",
      "Document 3:\n",
      "  Nodes: [Node(id='Graph Modeling', type='Concept', properties={}), Node(id='Systems Management Domain', type='Domain', properties={}), Node(id='Relational Modeling', type='Concept', properties={}), Node(id='Relational Databases', type='Technology', properties={}), Node(id='Graph Model', type='Model', properties={}), Node(id='Domain Model', type='Model', properties={}), Node(id='Application', type='Concept', properties={}), Node(id='Tables', type='Structure', properties={}), Node(id='Graph Database', type='Technology', properties={}), Node(id='Entity', type='Concept', properties={}), Node(id='Label', type='Concept', properties={}), Node(id='Property', type='Concept', properties={}), Node(id='Relationship', type='Concept', properties={}), Node(id='Node', type='Concept', properties={}), Node(id='Whiteboard Sketch', type='Method', properties={}), Node(id='Data Integrity', type='Concept', properties={}), Node(id='Rapid Change', type='Concept', properties={}), Node(id='Evolution', type='Concept', properties={}), Node(id='Performance', type='Concept', properties={}), Node(id='Rigid Schemas', type='Characteristic', properties={}), Node(id='Complex Modeling Characteristics', type='Characteristic', properties={}), Node(id='Growth', type='Concept', properties={}), Node(id='Domain', type='Concept', properties={}), Node(id='Application Goals', type='Goal', properties={}), Node(id='Domain Parts', type='Concept', properties={}), Node(id='Role', type='Concept', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Graph Modeling', type='Concept', properties={}), target=Node(id='Systems Management Domain', type='Domain', properties={}), type='APPLIES_TO', properties={}), Relationship(source=Node(id='Relational Modeling', type='Concept', properties={}), target=Node(id='Relational Databases', type='Technology', properties={}), type='USES', properties={}), Relationship(source=Node(id='Relational Databases', type='Technology', properties={}), target=Node(id='Rigid Schemas', type='Characteristic', properties={}), type='HAS', properties={}), Relationship(source=Node(id='Relational Databases', type='Technology', properties={}), target=Node(id='Complex Modeling Characteristics', type='Characteristic', properties={}), type='HAS', properties={}), Relationship(source=Node(id='Relational Databases', type='Technology', properties={}), target=Node(id='Rapid Change', type='Concept', properties={}), type='NOT_SUITABLE_FOR', properties={}), Relationship(source=Node(id='Graph Model', type='Model', properties={}), target=Node(id='Domain', type='Concept', properties={}), type='ALIGNS_WITH', properties={}), Relationship(source=Node(id='Graph Model', type='Model', properties={}), target=Node(id='Performance', type='Concept', properties={}), type='PRESERVES', properties={}), Relationship(source=Node(id='Graph Model', type='Model', properties={}), target=Node(id='Evolution', type='Concept', properties={}), type='SUPPORTS', properties={}), Relationship(source=Node(id='Graph Model', type='Model', properties={}), target=Node(id='Data Integrity', type='Concept', properties={}), type='MAINTAINS', properties={}), Relationship(source=Node(id='Graph Model', type='Model', properties={}), target=Node(id='Rapid Change', type='Concept', properties={}), type='SUPPORTS', properties={}), Relationship(source=Node(id='Graph Model', type='Model', properties={}), target=Node(id='Growth', type='Concept', properties={}), type='SUPPORTS', properties={}), Relationship(source=Node(id='Relational Modeling', type='Concept', properties={}), target=Node(id='Domain Model', type='Model', properties={}), type='TRANSFORMS', properties={}), Relationship(source=Node(id='Domain Model', type='Model', properties={}), target=Node(id='Tables', type='Structure', properties={}), type='BECOMES', properties={}), Relationship(source=Node(id='Graph Model', type='Model', properties={}), target=Node(id='Domain Model', type='Model', properties={}), type='ENRICHES', properties={}), Relationship(source=Node(id='Graph Model', type='Model', properties={}), target=Node(id='Domain Parts', type='Concept', properties={}), type='REPRESENTS', properties={}), Relationship(source=Node(id='Domain Parts', type='Concept', properties={}), target=Node(id='Application Goals', type='Goal', properties={}), type='RELEVANT_TO', properties={}), Relationship(source=Node(id='Entity', type='Concept', properties={}), target=Node(id='Label', type='Concept', properties={}), type='HAS', properties={}), Relationship(source=Node(id='Entity', type='Concept', properties={}), target=Node(id='Property', type='Concept', properties={}), type='HAS', properties={}), Relationship(source=Node(id='Entity', type='Concept', properties={}), target=Node(id='Relationship', type='Concept', properties={}), type='HAS', properties={}), Relationship(source=Node(id='Domain Modeling', type='Concept', properties={}), target=Node(id='Graph Modeling', type='Concept', properties={}), type='IS_ISOMORPHIC_TO', properties={}), Relationship(source=Node(id='Domain Model', type='Model', properties={}), target=Node(id='Graph Model', type='Model', properties={}), type='IMPROVES', properties={}), Relationship(source=Node(id='Graph Database', type='Technology', properties={}), target=Node(id='Whiteboard Sketch', type='Method', properties={}), type='STORES', properties={}), Relationship(source=Node(id='Graph Database', type='Technology', properties={}), target=Node(id='Graph Model', type='Model', properties={}), type='STORES', properties={}), Relationship(source=Node(id='Node', type='Concept', properties={}), target=Node(id='Role', type='Concept', properties={}), type='HAS', properties={})]\n",
      "---\n",
      "Document 4:\n",
      "  Nodes: [Node(id='Whiteboard Sketch', type='Concept', properties={}), Node(id='Graph Database', type='Database', properties={}), Node(id='Node', type='Concept', properties={}), Node(id='Role-Specific Label', type='Attribute', properties={}), Node(id='Property', type='Attribute', properties={}), Node(id='Data-Centric Domain Responsibility', type='Responsibility', properties={}), Node(id='Semantic Context', type='Concept', properties={}), Node(id='Relationship', type='Concept', properties={}), Node(id='Structural Aspect', type='Concept', properties={}), Node(id='Data Center Scenario', type='Scenario', properties={}), Node(id='Graph Model', type='Model', properties={}), Node(id='Figure 3-5', type='Figure', properties={}), Node(id='Chapter 3', type='Chapter', properties={}), Node(id='Data Modeling With Graphs', type='Topic', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Whiteboard Sketch', type='Concept', properties={}), target=Node(id='Graph Database', type='Database', properties={}), type='IS_STORED_IN', properties={}), Relationship(source=Node(id='Node', type='Concept', properties={}), target=Node(id='Role-Specific Label', type='Attribute', properties={}), type='HAS', properties={}), Relationship(source=Node(id='Node', type='Concept', properties={}), target=Node(id='Property', type='Attribute', properties={}), type='HAS', properties={}), Relationship(source=Node(id='Node', type='Concept', properties={}), target=Node(id='Data-Centric Domain Responsibility', type='Responsibility', properties={}), type='FULFILLS', properties={}), Relationship(source=Node(id='Node', type='Concept', properties={}), target=Node(id='Semantic Context', type='Concept', properties={}), type='IS_PLACED_IN', properties={}), Relationship(source=Node(id='Relationship', type='Concept', properties={}), target=Node(id='Structural Aspect', type='Concept', properties={}), type='CAPTURES', properties={}), Relationship(source=Node(id='Graph Model', type='Model', properties={}), target=Node(id='Data Center Scenario', type='Scenario', properties={}), type='APPLIES_TO', properties={}), Relationship(source=Node(id='Graph Model', type='Model', properties={}), target=Node(id='Figure 3-5', type='Figure', properties={}), type='IS_ILLUSTRATED_BY', properties={}), Relationship(source=Node(id='Chapter 3', type='Chapter', properties={}), target=Node(id='Data Modeling With Graphs', type='Topic', properties={}), type='COVERS', properties={})]\n",
      "---\n",
      "Document 5:\n",
      "  Nodes: [Node(id='Domain Model', type='Concept', properties={}), Node(id='Database', type='System', properties={}), Node(id='Database', type='Concept', properties={}), Node(id='App', type='Concept', properties={}), Node(id='Server', type='Concept', properties={}), Node(id='Asset', type='Concept', properties={}), Node(id='Query', type='Concept', properties={}), Node(id='Graph', type='Concept', properties={}), Node(id='Structure', type='Concept', properties={}), Node(id='Relational Modeling', type='Concept', properties={}), Node(id='Graph Modeling', type='Concept', properties={})]\n",
      "  Relationships: [Relationship(source=Node(id='Domain Model', type='Concept', properties={}), target=Node(id='Database', type='System', properties={}), type='IMPLEMENTED_IN', properties={}), Relationship(source=Node(id='Database', type='Concept', properties={}), target=Node(id='Asset', type='Concept', properties={}), type='IS_A', properties={}), Relationship(source=Node(id='App', type='Concept', properties={}), target=Node(id='Asset', type='Concept', properties={}), type='IS_A', properties={}), Relationship(source=Node(id='Server', type='Concept', properties={}), target=Node(id='Asset', type='Concept', properties={}), type='IS_A', properties={}), Relationship(source=Node(id='Query', type='Concept', properties={}), target=Node(id='Asset', type='Concept', properties={}), type='TARGETS', properties={}), Relationship(source=Node(id='Query', type='Concept', properties={}), target=Node(id='Database', type='Concept', properties={}), type='TARGETS', properties={}), Relationship(source=Node(id='Query', type='Concept', properties={}), target=Node(id='App', type='Concept', properties={}), type='TARGETS', properties={}), Relationship(source=Node(id='Query', type='Concept', properties={}), target=Node(id='Server', type='Concept', properties={}), type='TARGETS', properties={}), Relationship(source=Node(id='Graph', type='Concept', properties={}), target=Node(id='Structure', type='Concept', properties={}), type='SUPPORTS', properties={}), Relationship(source=Node(id='Relational Modeling', type='Concept', properties={}), target=Node(id='Graph Modeling', type='Concept', properties={}), type='COMPARED_WITH', properties={}), Relationship(source=Node(id='Domain Model', type='Concept', properties={}), target=Node(id='Query', type='Concept', properties={}), type='SUITABLE_FOR', properties={})]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# After converting to graph documents\n",
    "for i, doc in enumerate(graph_documents):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(f\"  Nodes: {doc.nodes}\")\n",
    "    print(f\"  Relationships: {doc.relationships}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     template=\"\"\"You are a grader assessing relevance \n",
    "#     of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "#     grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n",
    "    \n",
    "#     Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "#     Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "     \n",
    "#     Here is the retrieved document: \n",
    "#     {document}\n",
    "    \n",
    "#     Here is the user question: \n",
    "#     {question}\n",
    "#     \"\"\",\n",
    "#     input_variables=[\"question\", \"document\"],\n",
    "# )\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance\n",
    "of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
    "grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
    "\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "\n",
    "IMPORTANT: Your response MUST be only the JSON object itself, without any surrounding text or markdown.\n",
    "\n",
    "Here is the retrieved document:\n",
    "{document}\n",
    "\n",
    "Here is the user question:\n",
    "{question}\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"what is Step-back prompting\"\n",
    "docs = retriever.invoke(question)\n",
    "print(len(docs))\n",
    "print(docs,'\\n')\n",
    "doc_txt = docs[1].page_content\n",
    "print(\n",
    "    f'Is our answer relevant to the question asked: {retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[Document(metadata={'author': 'Tomaž Bratanic and Oskar Hane', 'creationDate': 'D:20250707132929Z', 'creationdate': '2025-07-07T13:29:29+00:00', 'creator': 'FrameMaker 16.0.1', 'file_path': './data_stored/Essential-GraphRAG.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20250714121407+02'00'\", 'moddate': '2025-07-14T12:14:07+02:00', 'page': 55, 'pk': 461791213351337984, 'producer': 'Acrobat Distiller 20.0 (Windows)', 'source': './data_stored/Essential-GraphRAG.pdf', 'subject': '', 'title': 'Essential GraphRAG', 'total_pages': 179, 'trapped': ''}, page_content='35\\n3.1\\nStep-back prompting\\nrange of information, making it easier for the model to identify relevant facts without\\ngetting bogged down by the specifics.\\n The authors used an LLM for the query rewriting task, as shown in figure 3.3.\\nLLMs are an excellent fit for query-rewriting tasks as they excel at natural language\\ncomprehension and generation. You don’t have to train or finetune a new model for\\neach task. Instead, you can provide task instructions in the input prompt.\\n The authors of the step-back prompting paper used the system prompt in the fol-\\nlowing listing to instruct the LLM on how to rewrite the input query.\\nstepback_system_message = f\"\"\"    \\nYou are an expert at world knowledge. Your task is to step back\\nand paraphrase a question to a more generic step-back question, which\\nis easier to answer. Here are a few examples\\n        \\n\"input\": \"Could the members of The Police perform lawful arrests?\"\\n\"output\": \"what can the members of The Police do?\"\\n\"input\": \"Jan Sindel’s was born in what country?\"\\n\"output\": \"what is Jan Sindel’s personal history?\"\\n\"\"\"\\nThe system prompt in listing 3.1 begins by giving the LLM a simple instruction to\\nrewrite a user’s question into a more generic, step-back version. On its own, this kind\\nof instruction is known as zero-shot prompting, which relies solely on the LLM’s general\\ncapabilities and understanding of the task, without providing any examples. However,\\nListing 3.1\\nSystem prompt of an LLM for generating step-back questions\\nOriginal question\\nRewritten\\nstep-back question\\nLLM with\\nstep-back prompt\\nWhich team did\\nThierry Audel play\\nfor from 2007 to 2008?\\nWhich teams did\\nThierry Audel play\\nfor in his career?\\nProcess with\\nLLM\\nStep-back output\\nFigure 3.3\\nRewriting queries using the step-back approach with an LLM\\nQuery \\nrewriting \\ninstructions\\nFew-shot \\nexamples'), Document(metadata={'author': 'Tomaž Bratanic and Oskar Hane', 'creationDate': 'D:20250707132929Z', 'creationdate': '2025-07-07T13:29:29+00:00', 'creator': 'FrameMaker 16.0.1', 'file_path': './data_stored/Essential-GraphRAG.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20250714121407+02'00'\", 'moddate': '2025-07-14T12:14:07+02:00', 'page': 56, 'pk': 461791213351337986, 'producer': 'Acrobat Distiller 20.0 (Windows)', 'source': './data_stored/Essential-GraphRAG.pdf', 'subject': '', 'title': 'Essential GraphRAG', 'total_pages': 179, 'trapped': ''}, page_content='The parent document retriever strategy involves dividing a large document into\\nsmaller sections, calculating embeddings for each section rather than the whole docu-\\nment, and using these embeddings to match user queries more accurately, ultimately\\nListing 3.2\\nFunction to generate a step-back question\\nListing 3.3\\nExecuting the step-back prompt function\\nExercise 3.1\\nTo explore the step-back prompt generation’s effectiveness, try applying it to various\\nquestions and observe how it broadens the context. You can also change the system\\nprompt to observe how it affects the output.'), Document(metadata={'author': 'Tomaž Bratanic and Oskar Hane', 'creationDate': 'D:20250707132929Z', 'creationdate': '2025-07-07T13:29:29+00:00', 'creator': 'FrameMaker 16.0.1', 'file_path': './data_stored/Essential-GraphRAG.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20250714121407+02'00'\", 'moddate': '2025-07-14T12:14:07+02:00', 'page': 56, 'pk': 461791213351337985, 'producer': 'Acrobat Distiller 20.0 (Windows)', 'source': './data_stored/Essential-GraphRAG.pdf', 'subject': '', 'title': 'Essential GraphRAG', 'total_pages': 179, 'trapped': ''}, page_content='36\\nCHAPTER 3\\nAdvanced vector retrieval strategies\\nto guide the model more effectively and ensure consistent results, the authors chose\\nto expand the prompt with several examples of the desired paraphrasing behavior.\\nThis technique is called few-shot prompting, where a small number of examples (typi-\\ncally two to five) are included in the prompt to illustrate the task. Few-shot prompting\\nhelps the LLM better understand the expected transformation by anchoring it in con-\\ncrete instances, improving the quality and reliability of the output.\\n To achieve the query rewriting, all you need to do is send the system prompt found\\nin listing 3.1 along with the user’s question to an LLM. The specific function for this\\ntask is outlined in the next listing.\\ndef generate_stepback(question: str):\\n    user_message = f\"\"\"{question}\"\"\"\\n    step_back_question = chat(\\n        messages=[\\n            {\"role\": \"system\", \"content\": stepback_system_message},\\n            {\"role\": \"user\", \"content\": user_message},\\n        ]\\n    )\\n    return step_back_question\\nYou can now test the step-back prompt generation by executing the code shown next.\\nquestion = \"Which team did Thierry Audel play for from 2007 to 2008?\"\\nstep_back_question = generate_stepback(question)\\nprint(f\"Stepback results: {step_back_question}\")\\n# Stepback results: What is the career history of Thierry Audel?\\nThe results in listing 3.3 demonstrate a successful execution of the step-back prompt\\ngeneration function. By transforming the specific query about Thierry Audel’s team\\nfrom 2007 to 2008 into a broader question regarding his entire career history, the\\nfunction effectively broadens the context and should increase the retrieval accuracy\\nand recall.\\n3.2\\nParent document retriever\\nThe parent document retriever strategy involves dividing a large document into\\nsmaller sections, calculating embeddings for each section rather than the whole docu-\\nment, and using these embeddings to match user queries more accurately, ultimately'), Document(metadata={'author': 'Ian Robinson', 'creationDate': 'D:20150430130949Z', 'creationdate': '2015-04-30T13:09:49+00:00', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'file_path': './data_stored/OReilly_Graph_Databases.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20150430091410-04'00'\", 'moddate': '2015-04-30T09:14:10-04:00', 'page': 55, 'pk': 461791213351337987, 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'source': './data_stored/OReilly_Graph_Databases.pdf', 'subject': '', 'title': 'Graph Databases', 'total_pages': 238, 'trapped': ''}, page_content='Graph Modeling in a Systems Management Domain\\nWe’ve seen how relational modeling and its attendant implementation activities take\\nus down a path that divorces an application’s underlying storage model from the con‐\\nceptual worldview of its stakeholders. Relational databases—with their rigid schemas\\nand complex modeling characteristics—are not an especially good tool for supporting\\nrapid change. What we need is a model that is closely aligned with the domain, but\\nthat doesn’t sacrifice performance, and that supports evolution while maintaining the\\nintegrity of the data as it undergoes rapid change and growth. That model is the\\ngraph model. How, then, does this process differ when realized with a graph data\\nmodel?\\nIn the early stages of analysis, the work required of us is similar to the relational\\napproach: using lo-fi methods, such as whiteboard sketches, we describe and agree\\nupon the domain. After that, however, the methodologies diverge. Instead of trans‐\\nforming a domain model’s graph-like representation into tables, we enrich it, with the\\naim of producing an accurate representation of the parts of the domain relevant to\\nour application goals. That is, for each entity in our domain, we ensure that we’ve\\ncaptured its relevant roles as labels, its attributes as properties, and its connections to\\nneighboring entities as relationships.\\nRemember, the domain model is not a transparent, context-free\\nwindow onto reality: rather, it is a purposeful abstraction of those\\naspects of our domain relevant to our application goals. There’s\\nalways some motivation for creating a model. By enriching our\\nfirst-cut domain graph with additional properties and relation‐\\nships, we effectively produce a graph model attuned to our applica‐\\ntion’s data needs; that is, we provide for answering the kinds of\\nquestions our application will ask of its data.\\nHelpfully, domain modeling is completely isomorphic to graph modeling. By ensur‐\\ning the correctness of the domain model, we’re implicitly improving the graph model,\\nbecause in a graph database what you sketch on the whiteboard is typically what you\\nstore in the database.\\nIn graph terms, what we’re doing is ensuring that each node has the appropriate role-')] \n",
      "\n",
      "Step-back prompting rewrites a question into a more generic one. This makes it easier for the model to identify relevant facts. It uses an LLM to transform a specific question into a broader one.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use six sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "question = \"what is Step-back prompting\"\n",
    "docs = retriever.invoke(question)\n",
    "print(len(docs))\n",
    "print(docs,'\\n')\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     39\u001b[39m graph_rag_chain = GraphCypherQAChain.from_llm(\n\u001b[32m     40\u001b[39m     cypher_llm=graph_llm,\n\u001b[32m     41\u001b[39m     qa_llm=llm,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     allow_dangerous_requests=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     50\u001b[39m )\n\u001b[32m     52\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mWhat is Step-back prompting\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m generation = \u001b[43mgraph_rag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(generation)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/langchain/chains/base.py:165\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    164\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    170\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    171\u001b[39m         inputs,\n\u001b[32m    172\u001b[39m         outputs,\n\u001b[32m    173\u001b[39m         return_only_outputs,\n\u001b[32m    174\u001b[39m     )\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/langchain_community/chains/graph_qa/cypher.py:373\u001b[39m, in \u001b[36mGraphCypherQAChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    369\u001b[39m args.update(inputs)\n\u001b[32m    371\u001b[39m intermediate_steps: List = []\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m generated_cypher = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcypher_generation_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# Extract Cypher code if it is wrapped in backticks\u001b[39;00m\n\u001b[32m    376\u001b[39m generated_cypher = extract_cypher(generated_cypher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:193\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    192\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/langchain/chains/base.py:627\u001b[39m, in \u001b[36mChain.run\u001b[39m\u001b[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    625\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33m`run` supports only one positional argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    626\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    628\u001b[39m         _output_key\n\u001b[32m    629\u001b[39m     ]\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    632\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[32m    633\u001b[39m         _output_key\n\u001b[32m    634\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:193\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    192\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/langchain/chains/base.py:410\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    379\u001b[39m \n\u001b[32m    380\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    401\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    403\u001b[39m config = {\n\u001b[32m    404\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    405\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    406\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    407\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    408\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/langchain/chains/base.py:165\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    164\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    170\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    171\u001b[39m         inputs,\n\u001b[32m    172\u001b[39m         outputs,\n\u001b[32m    173\u001b[39m         return_only_outputs,\n\u001b[32m    174\u001b[39m     )\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/langchain/chains/llm.py:127\u001b[39m, in \u001b[36mLLMChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    124\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    125\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    126\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_outputs(response)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/langchain/chains/llm.py:139\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    137\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m results = \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).batch(\n\u001b[32m    146\u001b[39m     cast(\u001b[38;5;28mlist\u001b[39m, prompts),\n\u001b[32m    147\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks},\n\u001b[32m    148\u001b[39m )\n\u001b[32m    149\u001b[39m generations: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Generation]] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1025\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     **kwargs: Any,\n\u001b[32m   1023\u001b[39m ) -> LLMResult:\n\u001b[32m   1024\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:842\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    841\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m         )\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    850\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1091\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1095\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:1790\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m   1789\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.max_retries\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1792\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1795\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1796\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:238\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    231\u001b[39m params = (\n\u001b[32m    232\u001b[39m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (request := kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[32m    237\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:208\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_with_retry\u001b[39m(**kwargs: Any) -> Any:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    210\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlocation is not supported\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m exc.message:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:869\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    866\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    868\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n\u001b[32m    149\u001b[39m             warnings.warn(_ASYNC_RETRY_WARNING)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    126\u001b[39m         remaining_timeout = \u001b[38;5;28mself\u001b[39m._timeout\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:75\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(callable_)\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_remapped_callable\u001b[39m(*args, **kwargs):\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/grpc/_interceptor.py:276\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    268\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    269\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    274\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    275\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/grpc/_interceptor.py:328\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m    326\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys.exc_info()[\u001b[32m2\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interceptor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call.result(), call\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py:78\u001b[39m, in \u001b[36m_LoggingClientInterceptor.intercept_unary_unary\u001b[39m\u001b[34m(self, continuation, client_call_details, request)\u001b[39m\n\u001b[32m     64\u001b[39m     grpc_request = {\n\u001b[32m     65\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpayload\u001b[39m\u001b[33m\"\u001b[39m: request_payload,\n\u001b[32m     66\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrequestMethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgrpc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     67\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[32m     68\u001b[39m     }\n\u001b[32m     69\u001b[39m     _LOGGER.debug(\n\u001b[32m     70\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details.method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m         extra={\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m         },\n\u001b[32m     77\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m response = \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[32m     80\u001b[39m     response_metadata = response.trailing_metadata()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/grpc/_interceptor.py:314\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    305\u001b[39m (\n\u001b[32m    306\u001b[39m     new_method,\n\u001b[32m    307\u001b[39m     new_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m    311\u001b[39m     new_compression,\n\u001b[32m    312\u001b[39m ) = _unwrap_client_call_details(new_details, client_call_details)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/grpc/_channel.py:1177\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1168\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_call\u001b[39m(\n\u001b[32m   1169\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1170\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1175\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1176\u001b[39m ) -> Tuple[Any, grpc.Call]:\n\u001b[32m-> \u001b[39m\u001b[32m1177\u001b[39m     state, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1180\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/graphrag_2/.venv/lib/python3.11/site-packages/grpc/_channel.py:1150\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._blocking\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1133\u001b[39m state.target = _common.decode(\u001b[38;5;28mself\u001b[39m._target)\n\u001b[32m   1134\u001b[39m call = \u001b[38;5;28mself\u001b[39m._channel.segregated_call(\n\u001b[32m   1135\u001b[39m     cygrpc.PropagationConstants.GRPC_PROPAGATE_DEFAULTS,\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28mself\u001b[39m._method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1148\u001b[39m     \u001b[38;5;28mself\u001b[39m._registered_call_handle,\n\u001b[32m   1149\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1150\u001b[39m event = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1151\u001b[39m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m._response_deserializer)\n\u001b[32m   1152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[39m, in \u001b[36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._internal_latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "### Graph Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "cypher_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert at generating Cypher queries for Neo4j.\n",
    "    Use the following schema to generate a Cypher query that answers the given question.\n",
    "    Make the query flexible by using case-insensitive matching and partial string matching where appropriate.\n",
    "    \n",
    "    \n",
    "    Schema:\n",
    "    {schema}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Cypher Query:\"\"\",\n",
    "    input_variables=[\"schema\", \"question\"],\n",
    ")\n",
    "\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following Cypher query results to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise. If topic information is not available, just say that topic information is not available.\n",
    "    \n",
    "    Question: {question} \n",
    "    Cypher Query: {query}\n",
    "    Query Results: {context} \n",
    "    \n",
    "    Answer:\"\"\",\n",
    "    input_variables=[\"question\", \"query\", \"context\"],\n",
    ")\n",
    "\n",
    "\n",
    "graph_rag_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm=graph_llm,\n",
    "    qa_llm=llm,\n",
    "    validate_cypher=True,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    "    return_direct=True,\n",
    "    cypher_prompt=cypher_prompt,\n",
    "    qa_prompt=qa_prompt,\n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "question = \"What is Step-back prompting\"\n",
    "generation = graph_rag_chain.invoke({\"query\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Composite Vector + Graph Generations\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context from a vector store and a graph database to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Vector Context: {context} \n",
    "    Graph Context: {graph_context}\n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"context\", \"graph_context\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Example input data\n",
    "question = \"what is Step-back prompting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'author': 'Tomaž Bratanic and Oskar Hane', 'creationDate': 'D:20250707132929Z', 'creationdate': '2025-07-07T13:29:29+00:00', 'creator': 'FrameMaker 16.0.1', 'file_path': './data_stored/Essential-GraphRAG.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20250714121407+02'00'\", 'moddate': '2025-07-14T12:14:07+02:00', 'page': 55, 'pk': 461791213351337984, 'producer': 'Acrobat Distiller 20.0 (Windows)', 'source': './data_stored/Essential-GraphRAG.pdf', 'subject': '', 'title': 'Essential GraphRAG', 'total_pages': 179, 'trapped': ''}, page_content='35\\n3.1\\nStep-back prompting\\nrange of information, making it easier for the model to identify relevant facts without\\ngetting bogged down by the specifics.\\n The authors used an LLM for the query rewriting task, as shown in figure 3.3.\\nLLMs are an excellent fit for query-rewriting tasks as they excel at natural language\\ncomprehension and generation. You don’t have to train or finetune a new model for\\neach task. Instead, you can provide task instructions in the input prompt.\\n The authors of the step-back prompting paper used the system prompt in the fol-\\nlowing listing to instruct the LLM on how to rewrite the input query.\\nstepback_system_message = f\"\"\"    \\nYou are an expert at world knowledge. Your task is to step back\\nand paraphrase a question to a more generic step-back question, which\\nis easier to answer. Here are a few examples\\n        \\n\"input\": \"Could the members of The Police perform lawful arrests?\"\\n\"output\": \"what can the members of The Police do?\"\\n\"input\": \"Jan Sindel’s was born in what country?\"\\n\"output\": \"what is Jan Sindel’s personal history?\"\\n\"\"\"\\nThe system prompt in listing 3.1 begins by giving the LLM a simple instruction to\\nrewrite a user’s question into a more generic, step-back version. On its own, this kind\\nof instruction is known as zero-shot prompting, which relies solely on the LLM’s general\\ncapabilities and understanding of the task, without providing any examples. However,\\nListing 3.1\\nSystem prompt of an LLM for generating step-back questions\\nOriginal question\\nRewritten\\nstep-back question\\nLLM with\\nstep-back prompt\\nWhich team did\\nThierry Audel play\\nfor from 2007 to 2008?\\nWhich teams did\\nThierry Audel play\\nfor in his career?\\nProcess with\\nLLM\\nStep-back output\\nFigure 3.3\\nRewriting queries using the step-back approach with an LLM\\nQuery \\nrewriting \\ninstructions\\nFew-shot \\nexamples'), Document(metadata={'author': 'Tomaž Bratanic and Oskar Hane', 'creationDate': 'D:20250707132929Z', 'creationdate': '2025-07-07T13:29:29+00:00', 'creator': 'FrameMaker 16.0.1', 'file_path': './data_stored/Essential-GraphRAG.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20250714121407+02'00'\", 'moddate': '2025-07-14T12:14:07+02:00', 'page': 56, 'pk': 461791213351337986, 'producer': 'Acrobat Distiller 20.0 (Windows)', 'source': './data_stored/Essential-GraphRAG.pdf', 'subject': '', 'title': 'Essential GraphRAG', 'total_pages': 179, 'trapped': ''}, page_content='The parent document retriever strategy involves dividing a large document into\\nsmaller sections, calculating embeddings for each section rather than the whole docu-\\nment, and using these embeddings to match user queries more accurately, ultimately\\nListing 3.2\\nFunction to generate a step-back question\\nListing 3.3\\nExecuting the step-back prompt function\\nExercise 3.1\\nTo explore the step-back prompt generation’s effectiveness, try applying it to various\\nquestions and observe how it broadens the context. You can also change the system\\nprompt to observe how it affects the output.'), Document(metadata={'author': 'Tomaž Bratanic and Oskar Hane', 'creationDate': 'D:20250707132929Z', 'creationdate': '2025-07-07T13:29:29+00:00', 'creator': 'FrameMaker 16.0.1', 'file_path': './data_stored/Essential-GraphRAG.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20250714121407+02'00'\", 'moddate': '2025-07-14T12:14:07+02:00', 'page': 56, 'pk': 461791213351337985, 'producer': 'Acrobat Distiller 20.0 (Windows)', 'source': './data_stored/Essential-GraphRAG.pdf', 'subject': '', 'title': 'Essential GraphRAG', 'total_pages': 179, 'trapped': ''}, page_content='36\\nCHAPTER 3\\nAdvanced vector retrieval strategies\\nto guide the model more effectively and ensure consistent results, the authors chose\\nto expand the prompt with several examples of the desired paraphrasing behavior.\\nThis technique is called few-shot prompting, where a small number of examples (typi-\\ncally two to five) are included in the prompt to illustrate the task. Few-shot prompting\\nhelps the LLM better understand the expected transformation by anchoring it in con-\\ncrete instances, improving the quality and reliability of the output.\\n To achieve the query rewriting, all you need to do is send the system prompt found\\nin listing 3.1 along with the user’s question to an LLM. The specific function for this\\ntask is outlined in the next listing.\\ndef generate_stepback(question: str):\\n    user_message = f\"\"\"{question}\"\"\"\\n    step_back_question = chat(\\n        messages=[\\n            {\"role\": \"system\", \"content\": stepback_system_message},\\n            {\"role\": \"user\", \"content\": user_message},\\n        ]\\n    )\\n    return step_back_question\\nYou can now test the step-back prompt generation by executing the code shown next.\\nquestion = \"Which team did Thierry Audel play for from 2007 to 2008?\"\\nstep_back_question = generate_stepback(question)\\nprint(f\"Stepback results: {step_back_question}\")\\n# Stepback results: What is the career history of Thierry Audel?\\nThe results in listing 3.3 demonstrate a successful execution of the step-back prompt\\ngeneration function. By transforming the specific query about Thierry Audel’s team\\nfrom 2007 to 2008 into a broader question regarding his entire career history, the\\nfunction effectively broadens the context and should increase the retrieval accuracy\\nand recall.\\n3.2\\nParent document retriever\\nThe parent document retriever strategy involves dividing a large document into\\nsmaller sections, calculating embeddings for each section rather than the whole docu-\\nment, and using these embeddings to match user queries more accurately, ultimately'), Document(metadata={'author': 'Ian Robinson', 'creationDate': 'D:20150430130949Z', 'creationdate': '2015-04-30T13:09:49+00:00', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'file_path': './data_stored/OReilly_Graph_Databases.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20150430091410-04'00'\", 'moddate': '2015-04-30T09:14:10-04:00', 'page': 55, 'pk': 461791213351337987, 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'source': './data_stored/OReilly_Graph_Databases.pdf', 'subject': '', 'title': 'Graph Databases', 'total_pages': 238, 'trapped': ''}, page_content='Graph Modeling in a Systems Management Domain\\nWe’ve seen how relational modeling and its attendant implementation activities take\\nus down a path that divorces an application’s underlying storage model from the con‐\\nceptual worldview of its stakeholders. Relational databases—with their rigid schemas\\nand complex modeling characteristics—are not an especially good tool for supporting\\nrapid change. What we need is a model that is closely aligned with the domain, but\\nthat doesn’t sacrifice performance, and that supports evolution while maintaining the\\nintegrity of the data as it undergoes rapid change and growth. That model is the\\ngraph model. How, then, does this process differ when realized with a graph data\\nmodel?\\nIn the early stages of analysis, the work required of us is similar to the relational\\napproach: using lo-fi methods, such as whiteboard sketches, we describe and agree\\nupon the domain. After that, however, the methodologies diverge. Instead of trans‐\\nforming a domain model’s graph-like representation into tables, we enrich it, with the\\naim of producing an accurate representation of the parts of the domain relevant to\\nour application goals. That is, for each entity in our domain, we ensure that we’ve\\ncaptured its relevant roles as labels, its attributes as properties, and its connections to\\nneighboring entities as relationships.\\nRemember, the domain model is not a transparent, context-free\\nwindow onto reality: rather, it is a purposeful abstraction of those\\naspects of our domain relevant to our application goals. There’s\\nalways some motivation for creating a model. By enriching our\\nfirst-cut domain graph with additional properties and relation‐\\nships, we effectively produce a graph model attuned to our applica‐\\ntion’s data needs; that is, we provide for answering the kinds of\\nquestions our application will ask of its data.\\nHelpfully, domain modeling is completely isomorphic to graph modeling. By ensur‐\\ning the correctness of the domain model, we’re implicitly improving the graph model,\\nbecause in a graph database what you sketch on the whiteboard is typically what you\\nstore in the database.\\nIn graph terms, what we’re doing is ensuring that each node has the appropriate role-')]\n"
     ]
    }
   ],
   "source": [
    "# Get vector + graph answers\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "print(len(docs))\n",
    "print(docs,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-back prompting rewrites a question into a more generic one. This makes it easier for the model to identify relevant facts. It uses an LLM to transform a specific query into a broader question.\n"
     ]
    }
   ],
   "source": [
    "vector_context = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "\n",
    "print(vector_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N52', status_description='warn: property key does not exist. The property `abstract` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=4, column=19, offset=104>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 104, 'line': 4, 'column': 19}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: \"cypher\\nMATCH (p:Paper)\\nWHERE TOLOWER(p.title) CONTAINS TOLOWER('Step-back prompting')\\nRETURN p.title, p.abstract, p.url\\n\"\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N52', status_description='warn: property key does not exist. The property `url` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=4, column=31, offset=116>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 116, 'line': 4, 'column': 31}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: \"cypher\\nMATCH (p:Paper)\\nWHERE TOLOWER(p.title) CONTAINS TOLOWER('Step-back prompting')\\nRETURN p.title, p.abstract, p.url\\n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (p:Paper)\n",
      "WHERE TOLOWER(p.title) CONTAINS TOLOWER('Step-back prompting')\n",
      "RETURN p.title, p.abstract, p.url\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'what is Step-back prompting', 'result': [], 'intermediate_steps': [{'query': \"cypher\\nMATCH (p:Paper)\\nWHERE TOLOWER(p.title) CONTAINS TOLOWER('Step-back prompting')\\nRETURN p.title, p.abstract, p.url\\n\"}]}\n"
     ]
    }
   ],
   "source": [
    "graph_context = graph_rag_chain.invoke({\"query\": question})\n",
    "\n",
    "print(graph_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-back prompting rewrites a question into a more generic one. This helps the model identify relevant facts. It uses an LLM to transform a specific query into a broader question.\n"
     ]
    }
   ],
   "source": [
    "composite_chain = prompt | llm | StrOutputParser()\n",
    "answer = composite_chain.invoke(\n",
    "    {\"question\": question, \"context\": vector_context, \"graph_context\": graph_context}\n",
    ")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preamble or explanation.\n",
    "    \n",
    "    Here are the facts:\n",
    "    {documents} \n",
    "\n",
    "    Here is the answer: \n",
    "    {generation}\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'no'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "     \n",
    "    Here is the answer:\n",
    "    {generation} \n",
    "\n",
    "    Here is the question: {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_672463/3223396883.py:28: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'vectorstore'}\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert at routing a user question to the most appropriate data source. \n",
    "    You have three options:\n",
    "    1. 'vectorstore': Use for questions about explanations of topics.\n",
    "    2. 'graphrag': Use for questions that involve relationships between concepts.\n",
    "    3. 'web_search': Use for all other questions or when current information is needed.\n",
    "\n",
    "    You do not need to be stringent with the keywords in the question related to these topics. \n",
    "    Choose the most appropriate option based on the nature of the question.\n",
    "\n",
    "    Return a JSON with a single key 'datasource' and no preamble or explanation. \n",
    "    The value should be one of: 'vectorstore', 'graphrag', or 'web_search'.\n",
    "    \n",
    "    Question to route: \n",
    "    {question}\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "question = \"what is Step-back prompting\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_672463/730760015.py:5: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  web_search_tool = TavilySearchResults(k=3)\n"
     ]
    }
   ],
   "source": [
    "### Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement these as a control flow in LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7a3296927f90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "### State\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "        graph_context: results from graph search\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "    graph_context: str\n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "### Nodes\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents and graph context\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    graph_context = state.get(\"graph_context\", \"\")\n",
    "\n",
    "    # Composite RAG generation\n",
    "    generation = composite_chain.invoke(\n",
    "        {\"question\": question, \"context\": documents, \"graph_context\": graph_context}\n",
    "    )\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"generation\": generation,\n",
    "        \"graph_context\": graph_context,\n",
    "    }\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])  # Use get() with a default empty list\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "### Conditional edge\n",
    "def route_question(state):\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    print(source)\n",
    "    print(source[\"datasource\"])\n",
    "\n",
    "    if source[\"datasource\"] == \"graphrag\":\n",
    "        print(\"---TRYING GRAPH SEARCH---\")\n",
    "        graph_result = graph_search({\"question\": question})\n",
    "        if graph_result[\"graph_context\"] != \"No results found in the graph database.\":\n",
    "            return \"graphrag\"\n",
    "        else:\n",
    "            print(\"---NO RESULTS IN GRAPH, FALLING BACK TO VECTORSTORE---\")\n",
    "            return \"retrieve\"\n",
    "    elif source[\"datasource\"] == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO VECTORSTORE RAG---\")\n",
    "        return \"retrieve\"\n",
    "    elif source[\"datasource\"] == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def graph_search(state):\n",
    "    \"\"\"\n",
    "    Perform GraphRAG search using Neo4j\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updated state with graph search results\n",
    "    \"\"\"\n",
    "    print(\"---GRAPH SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Use the graph_rag_chain to perform the search\n",
    "    result = graph_rag_chain.invoke({\"query\": question})\n",
    "\n",
    "    # Extract the relevant information from the result\n",
    "    # Adjust this based on what graph_rag_chain returns\n",
    "    graph_context = result.get(\"result\", \"\")\n",
    "\n",
    "    # You might want to combine this with existing documents or keep it separate\n",
    "    return {\"graph_context\": graph_context, \"question\": question}\n",
    "\n",
    "\n",
    "### Conditional edge\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = grade = score.get(\"score\", \"\").lower()\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"graphrag\", graph_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7a3296927f90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set conditional entry point\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"graphrag\": \"graphrag\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_edge(\"graphrag\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "what is Step-back prompting?\n",
      "{'datasource': 'vectorstore'}\n",
      "vectorstore\n",
      "---ROUTE QUESTION TO VECTORSTORE RAG---\n",
      "---RETRIEVE---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\n",
      "'Finished running: grade_documents:'\n",
      "---WEB SEARCH---\n",
      "'Finished running: websearch:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "'Finished running: generate:'\n",
      "('Step-back prompting is a technique that encourages a language model to '\n",
      " 'consider abstractions and first principles before answering a question. It '\n",
      " 'involves two steps: abstraction, where the model identifies higher-level '\n",
      " 'concepts, and reasoning, where it uses those concepts to answer the '\n",
      " 'question. This approach can improve the accuracy of complex reasoning tasks.')\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"what is Step-back prompting?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile\n",
    "# app = workflow.compile()\n",
    "\n",
    "# # Test\n",
    "# from pprint import pprint\n",
    "\n",
    "# inputs = {\"question\": \"Did Emmanuel Macron visit Germany recently?\"}\n",
    "# for output in app.stream(inputs):\n",
    "#     for key, value in output.items():\n",
    "#         pprint(f\"Finished running: {key}:\")\n",
    "# pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test\n",
    "# from pprint import pprint\n",
    "\n",
    "# inputs = {\"question\": \"Which paper talk about large language models?\"}\n",
    "# for output in app.stream(inputs):\n",
    "#     for key, value in output.items():\n",
    "#         pprint(f\"Finished running: {key}:\")\n",
    "# pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
