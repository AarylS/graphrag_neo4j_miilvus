{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "set_debug(False)\n",
    "set_verbose(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "\n",
    "llm_name = \"gemini-2.0-flash-lite\"     \n",
    "graph_llm_name = \"gemini-2.5-flash-lite\" \n",
    "embed_model_name = \"all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fuths/projects/graphrag_2/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# embedding model\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# model names\n",
    "# model_name=\"all-MiniLM-L6-v2\"\n",
    "# model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph llm - cypher generation\n",
    "\n",
    "# google gemini\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "graph_llm = ChatGoogleGenerativeAI(model=graph_llm_name, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_672463/355719928.py:5: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"qwen3:4b\", format=\"json\", temperature=0)\n"
     ]
    }
   ],
   "source": [
    "# answering llm\n",
    "\n",
    "# gemini\n",
    "llm = ChatGoogleGenerativeAI(model=llm_name, temperature=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama qwen3 4b\n",
    "# from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# llm = ChatOllama(model=\"qwen3:4b\", format=\"json\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [does answer generation, rest of the work] llm - huggingface model\n",
    "\n",
    "# from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=repo_id,\n",
    "#     max_new_tokens=1024,\n",
    "#     do_sample=False,\n",
    "#     temperature=0,\n",
    "#     provider=\"auto\",  # let Hugging Face choose the best provider for you\n",
    "# )\n",
    "\n",
    "# chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# \"\"\" usage example\n",
    "\n",
    "# messages = \"tell a joke\"\n",
    "# ai_msg = chat_model.invoke(messages)\n",
    "\n",
    "# print(ai_msg.content)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDFs from: ./data_stored/\n",
      "  - Loading document: Essential-GraphRAG.pdf\n",
      "  - Loading document: OReilly_Graph_Databases.pdf\n",
      "Loaded a total of 4 pages from 2 PDF files.\n"
     ]
    }
   ],
   "source": [
    "# get and load documents\n",
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf_folder_path = \"./data_stored/\"\n",
    "print(f\"Loading PDFs from: {pdf_folder_path}\")\n",
    "\n",
    "pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith(\".pdf\")]\n",
    "\n",
    "all_documents = []\n",
    "for pdf_file in pdf_files:\n",
    "    file_path = os.path.join(pdf_folder_path, pdf_file)\n",
    "    print(f\"  - Loading document: {pdf_file}\")\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "\n",
    "    \"\"\" uncomment the lines below for the demo example \n",
    "    start_index = 55  \n",
    "    end_index = 57   \n",
    "\n",
    "    cleaned_pages = [\n",
    "        page for page in pages \n",
    "        if start_index <= page.metadata.get('page', 0) < end_index\n",
    "    ]\n",
    "\n",
    "    all_documents.extend(cleaned_pages)\n",
    "    \"\"\"\n",
    "\n",
    "    # comment the line below for the demo example\n",
    "    all_documents.extend(pages)\n",
    "\n",
    "print(f\"Loaded a total of {len(all_documents)} pages from {len(pdf_files)} PDF files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the documents into 6 chunks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the loaded documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"Split the documents into {len(doc_splits)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fuths/projects/graphrag_2/.venv/lib/python3.11/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    }
   ],
   "source": [
    "# creating vectordb\n",
    "\n",
    "from langchain_milvus import Milvus\n",
    "\n",
    "# Add to Milvus\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag_milvus_att_5\",\n",
    "    embedding=embedding_model,\n",
    "    connection_args={\"uri\": \"./milvus_ingest_att_5.db\"},\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_672463/4244973288.py:12: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
      "  graph = Neo4jGraph()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "end of chunk, time taken: 16.40524172782898\n",
      "end of chunk, time taken: 30.17276668548584\n",
      "end of chunk, time taken: 23.614866733551025\n",
      "end of chunk, time taken: 40.06956386566162\n",
      "end of chunk, time taken: 32.98189043998718\n",
      "end of chunk, time taken: 22.406184673309326\n",
      "2\n",
      "3\n",
      "Graph documents: 6\n",
      "Nodes from 1st graph doc:[Node(id='Step-Back Prompting', type='Technique', properties={}), Node(id='Llm', type='Model', properties={}), Node(id='Query Rewriting Task', type='Task', properties={}), Node(id='Natural Language Comprehension', type='Capability', properties={}), Node(id='Natural Language Generation', type='Capability', properties={}), Node(id='Authors Of The Step-Back Prompting Paper', type='Group', properties={}), Node(id='System Prompt', type='Instruction', properties={}), Node(id='Zero-Shot Prompting', type='Technique', properties={}), Node(id='Thierry Audel', type='Person', properties={}), Node(id='Few-Shot Examples', type='Concept', properties={}), Node(id='General Capabilities', type='Capability', properties={})]\n",
      "Relationships from 1st graph doc:[Relationship(source=Node(id='Step-Back Prompting', type='Technique', properties={}), target=Node(id='Llm', type='Model', properties={}), type='USES', properties={}), Relationship(source=Node(id='Llm', type='Model', properties={}), target=Node(id='Query Rewriting Task', type='Task', properties={}), type='PERFORMS', properties={}), Relationship(source=Node(id='Llm', type='Model', properties={}), target=Node(id='Natural Language Comprehension', type='Capability', properties={}), type='EXCELS_AT', properties={}), Relationship(source=Node(id='Llm', type='Model', properties={}), target=Node(id='Natural Language Generation', type='Capability', properties={}), type='EXCELS_AT', properties={}), Relationship(source=Node(id='Authors Of The Step-Back Prompting Paper', type='Group', properties={}), target=Node(id='System Prompt', type='Instruction', properties={}), type='USED', properties={}), Relationship(source=Node(id='System Prompt', type='Instruction', properties={}), target=Node(id='Llm', type='Model', properties={}), type='INSTRUCTS', properties={}), Relationship(source=Node(id='System Prompt', type='Instruction', properties={}), target=Node(id='Few-Shot Examples', type='Concept', properties={}), type='CONTAINS', properties={}), Relationship(source=Node(id='Zero-Shot Prompting', type='Technique', properties={}), target=Node(id='Llm', type='Model', properties={}), type='RELY_ON', properties={}), Relationship(source=Node(id='Llm', type='Model', properties={}), target=Node(id='General Capabilities', type='Capability', properties={}), type='HAS', properties={}), Relationship(source=Node(id='Thierry Audel', type='Person', properties={}), target=Node(id='Few-Shot Examples', type='Concept', properties={}), type='IS_EXAMPLE_IN', properties={})]\n"
     ]
    }
   ],
   "source": [
    "# GraphRAG Setup\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "\n",
    "\n",
    "graph_transformer = LLMGraphTransformer(\n",
    "    llm=graph_llm,\n",
    "    # allowed_nodes=[\"Concept\"],\n",
    "    # node_properties=[\"\"],\n",
    "    # allowed_relationships=[\"AUTHORED\", \"\", \"RELATED_TO\"],\n",
    ")\n",
    "\n",
    "graph_documents = []\n",
    "for i,doc in enumerate(doc_splits):\n",
    "    stime = time.time()\n",
    "    # Process one document at a time\n",
    "    graph_doc = graph_transformer.convert_to_graph_documents([doc])\n",
    "    graph_documents.extend(graph_doc)\n",
    "    print(f\"end of chunk, time taken: {time.time() - stime}\")\n",
    "    \n",
    "\n",
    "graph.add_graph_documents(graph_documents)\n",
    "print(f\"Graph documents: {len(graph_documents)}\")\n",
    "print(f\"Nodes from 1st graph doc:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships from 1st graph doc:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" for testing graph\n",
    "# After converting to graph documents\n",
    "for i, doc in enumerate(graph_documents):\n",
    "    print(f\"Document {i}:\")\n",
    "    print(f\"  Nodes: {doc.nodes}\")\n",
    "    print(f\"  Relationships: {doc.relationships}\")\n",
    "    print(\"---\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n",
    "    \n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "     \n",
    "    Here is the retrieved document: \n",
    "    {document}\n",
    "    \n",
    "    Here is the user question: \n",
    "    {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     template=\"\"\"You are a grader assessing relevance\n",
    "# of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
    "# grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
    "\n",
    "# Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "# Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "\n",
    "# IMPORTANT: Your response MUST be only the JSON object itself, without any surrounding text or markdown.\n",
    "\n",
    "# Here is the retrieved document:\n",
    "# {document}\n",
    "\n",
    "# Here is the user question:\n",
    "# {question}\n",
    "# \"\"\",\n",
    "#     input_variables=[\"question\", \"document\"],\n",
    "# )\n",
    "\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "\"\"\" test retriever usage\n",
    "question = \"what is Step-back prompting\"\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "doc_txt = docs[1].page_content\n",
    "print(\n",
    "    f'Is our answer relevant to the question asked: {retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})}'\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[Document(metadata={'author': 'Tomaž Bratanic and Oskar Hane', 'creationDate': 'D:20250707132929Z', 'creationdate': '2025-07-07T13:29:29+00:00', 'creator': 'FrameMaker 16.0.1', 'file_path': './data_stored/Essential-GraphRAG.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20250714121407+02'00'\", 'moddate': '2025-07-14T12:14:07+02:00', 'page': 55, 'pk': 461791213351337984, 'producer': 'Acrobat Distiller 20.0 (Windows)', 'source': './data_stored/Essential-GraphRAG.pdf', 'subject': '', 'title': 'Essential GraphRAG', 'total_pages': 179, 'trapped': ''}, page_content='35\\n3.1\\nStep-back prompting\\nrange of information, making it easier for the model to identify relevant facts without\\ngetting bogged down by the specifics.\\n The authors used an LLM for the query rewriting task, as shown in figure 3.3.\\nLLMs are an excellent fit for query-rewriting tasks as they excel at natural language\\ncomprehension and generation. You don’t have to train or finetune a new model for\\neach task. Instead, you can provide task instructions in the input prompt.\\n The authors of the step-back prompting paper used the system prompt in the fol-\\nlowing listing to instruct the LLM on how to rewrite the input query.\\nstepback_system_message = f\"\"\"    \\nYou are an expert at world knowledge. Your task is to step back\\nand paraphrase a question to a more generic step-back question, which\\nis easier to answer. Here are a few examples\\n        \\n\"input\": \"Could the members of The Police perform lawful arrests?\"\\n\"output\": \"what can the members of The Police do?\"\\n\"input\": \"Jan Sindel’s was born in what country?\"\\n\"output\": \"what is Jan Sindel’s personal history?\"\\n\"\"\"\\nThe system prompt in listing 3.1 begins by giving the LLM a simple instruction to\\nrewrite a user’s question into a more generic, step-back version. On its own, this kind\\nof instruction is known as zero-shot prompting, which relies solely on the LLM’s general\\ncapabilities and understanding of the task, without providing any examples. However,\\nListing 3.1\\nSystem prompt of an LLM for generating step-back questions\\nOriginal question\\nRewritten\\nstep-back question\\nLLM with\\nstep-back prompt\\nWhich team did\\nThierry Audel play\\nfor from 2007 to 2008?\\nWhich teams did\\nThierry Audel play\\nfor in his career?\\nProcess with\\nLLM\\nStep-back output\\nFigure 3.3\\nRewriting queries using the step-back approach with an LLM\\nQuery \\nrewriting \\ninstructions\\nFew-shot \\nexamples'), Document(metadata={'author': 'Tomaž Bratanic and Oskar Hane', 'creationDate': 'D:20250707132929Z', 'creationdate': '2025-07-07T13:29:29+00:00', 'creator': 'FrameMaker 16.0.1', 'file_path': './data_stored/Essential-GraphRAG.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20250714121407+02'00'\", 'moddate': '2025-07-14T12:14:07+02:00', 'page': 56, 'pk': 461791213351337986, 'producer': 'Acrobat Distiller 20.0 (Windows)', 'source': './data_stored/Essential-GraphRAG.pdf', 'subject': '', 'title': 'Essential GraphRAG', 'total_pages': 179, 'trapped': ''}, page_content='The parent document retriever strategy involves dividing a large document into\\nsmaller sections, calculating embeddings for each section rather than the whole docu-\\nment, and using these embeddings to match user queries more accurately, ultimately\\nListing 3.2\\nFunction to generate a step-back question\\nListing 3.3\\nExecuting the step-back prompt function\\nExercise 3.1\\nTo explore the step-back prompt generation’s effectiveness, try applying it to various\\nquestions and observe how it broadens the context. You can also change the system\\nprompt to observe how it affects the output.'), Document(metadata={'author': 'Tomaž Bratanic and Oskar Hane', 'creationDate': 'D:20250707132929Z', 'creationdate': '2025-07-07T13:29:29+00:00', 'creator': 'FrameMaker 16.0.1', 'file_path': './data_stored/Essential-GraphRAG.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20250714121407+02'00'\", 'moddate': '2025-07-14T12:14:07+02:00', 'page': 56, 'pk': 461791213351337985, 'producer': 'Acrobat Distiller 20.0 (Windows)', 'source': './data_stored/Essential-GraphRAG.pdf', 'subject': '', 'title': 'Essential GraphRAG', 'total_pages': 179, 'trapped': ''}, page_content='36\\nCHAPTER 3\\nAdvanced vector retrieval strategies\\nto guide the model more effectively and ensure consistent results, the authors chose\\nto expand the prompt with several examples of the desired paraphrasing behavior.\\nThis technique is called few-shot prompting, where a small number of examples (typi-\\ncally two to five) are included in the prompt to illustrate the task. Few-shot prompting\\nhelps the LLM better understand the expected transformation by anchoring it in con-\\ncrete instances, improving the quality and reliability of the output.\\n To achieve the query rewriting, all you need to do is send the system prompt found\\nin listing 3.1 along with the user’s question to an LLM. The specific function for this\\ntask is outlined in the next listing.\\ndef generate_stepback(question: str):\\n    user_message = f\"\"\"{question}\"\"\"\\n    step_back_question = chat(\\n        messages=[\\n            {\"role\": \"system\", \"content\": stepback_system_message},\\n            {\"role\": \"user\", \"content\": user_message},\\n        ]\\n    )\\n    return step_back_question\\nYou can now test the step-back prompt generation by executing the code shown next.\\nquestion = \"Which team did Thierry Audel play for from 2007 to 2008?\"\\nstep_back_question = generate_stepback(question)\\nprint(f\"Stepback results: {step_back_question}\")\\n# Stepback results: What is the career history of Thierry Audel?\\nThe results in listing 3.3 demonstrate a successful execution of the step-back prompt\\ngeneration function. By transforming the specific query about Thierry Audel’s team\\nfrom 2007 to 2008 into a broader question regarding his entire career history, the\\nfunction effectively broadens the context and should increase the retrieval accuracy\\nand recall.\\n3.2\\nParent document retriever\\nThe parent document retriever strategy involves dividing a large document into\\nsmaller sections, calculating embeddings for each section rather than the whole docu-\\nment, and using these embeddings to match user queries more accurately, ultimately'), Document(metadata={'author': 'Ian Robinson', 'creationDate': 'D:20150430130949Z', 'creationdate': '2015-04-30T13:09:49+00:00', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'file_path': './data_stored/OReilly_Graph_Databases.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20150430091410-04'00'\", 'moddate': '2015-04-30T09:14:10-04:00', 'page': 55, 'pk': 461791213351337987, 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'source': './data_stored/OReilly_Graph_Databases.pdf', 'subject': '', 'title': 'Graph Databases', 'total_pages': 238, 'trapped': ''}, page_content='Graph Modeling in a Systems Management Domain\\nWe’ve seen how relational modeling and its attendant implementation activities take\\nus down a path that divorces an application’s underlying storage model from the con‐\\nceptual worldview of its stakeholders. Relational databases—with their rigid schemas\\nand complex modeling characteristics—are not an especially good tool for supporting\\nrapid change. What we need is a model that is closely aligned with the domain, but\\nthat doesn’t sacrifice performance, and that supports evolution while maintaining the\\nintegrity of the data as it undergoes rapid change and growth. That model is the\\ngraph model. How, then, does this process differ when realized with a graph data\\nmodel?\\nIn the early stages of analysis, the work required of us is similar to the relational\\napproach: using lo-fi methods, such as whiteboard sketches, we describe and agree\\nupon the domain. After that, however, the methodologies diverge. Instead of trans‐\\nforming a domain model’s graph-like representation into tables, we enrich it, with the\\naim of producing an accurate representation of the parts of the domain relevant to\\nour application goals. That is, for each entity in our domain, we ensure that we’ve\\ncaptured its relevant roles as labels, its attributes as properties, and its connections to\\nneighboring entities as relationships.\\nRemember, the domain model is not a transparent, context-free\\nwindow onto reality: rather, it is a purposeful abstraction of those\\naspects of our domain relevant to our application goals. There’s\\nalways some motivation for creating a model. By enriching our\\nfirst-cut domain graph with additional properties and relation‐\\nships, we effectively produce a graph model attuned to our applica‐\\ntion’s data needs; that is, we provide for answering the kinds of\\nquestions our application will ask of its data.\\nHelpfully, domain modeling is completely isomorphic to graph modeling. By ensur‐\\ning the correctness of the domain model, we’re implicitly improving the graph model,\\nbecause in a graph database what you sketch on the whiteboard is typically what you\\nstore in the database.\\nIn graph terms, what we’re doing is ensuring that each node has the appropriate role-')] \n",
      "\n",
      "Step-back prompting rewrites a question into a more generic one. This makes it easier for the model to identify relevant facts. It uses an LLM to transform a specific question into a broader one.\n"
     ]
    }
   ],
   "source": [
    "### answer Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use six sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\"\"\" test retriever usage\n",
    "question = \"what is Step-back prompting\"\n",
    "docs = retriever.invoke(question)\n",
    "print(len(docs))\n",
    "print(docs,'\\n')\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "\n",
    "\n",
    "cypher_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert at generating Cypher queries for Neo4j.\n",
    "    Use the following schema to generate a Cypher query that answers the given question.\n",
    "    Make the query flexible by using case-insensitive matching and partial string matching where appropriate.\n",
    "    \n",
    "    \n",
    "    Schema:\n",
    "    {schema}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Cypher Query:\"\"\",\n",
    "    input_variables=[\"schema\", \"question\"],\n",
    ")\n",
    "\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following Cypher query results to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise. If topic information is not available, just say that topic information is not available.\n",
    "    \n",
    "    Question: {question} \n",
    "    Cypher Query: {query}\n",
    "    Query Results: {context} \n",
    "    \n",
    "    Answer:\"\"\",\n",
    "    input_variables=[\"question\", \"query\", \"context\"],\n",
    ")\n",
    "\n",
    "\n",
    "graph_rag_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm=graph_llm,\n",
    "    qa_llm=llm,\n",
    "    validate_cypher=True,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    "    return_direct=True,\n",
    "    cypher_prompt=cypher_prompt,\n",
    "    qa_prompt=qa_prompt,\n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "\"\"\" testing Graph Generate\n",
    "question = \"What is Step-back prompting\"\n",
    "generation = graph_rag_chain.invoke({\"query\": question})\n",
    "print(generation)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### graph rag.\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context from a vector store and a graph database to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Vector Context: {context} \n",
    "    Graph Context: {graph_context}\n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"context\", \"graph_context\"],\n",
    ")\n",
    "\n",
    "composite_chain = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'author': 'Tomaž Bratanic and Oskar Hane', 'creationDate': 'D:20250707132929Z', 'creationdate': '2025-07-07T13:29:29+00:00', 'creator': 'FrameMaker 16.0.1', 'file_path': './data_stored/Essential-GraphRAG.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20250714121407+02'00'\", 'moddate': '2025-07-14T12:14:07+02:00', 'page': 55, 'pk': 461791213351337984, 'producer': 'Acrobat Distiller 20.0 (Windows)', 'source': './data_stored/Essential-GraphRAG.pdf', 'subject': '', 'title': 'Essential GraphRAG', 'total_pages': 179, 'trapped': ''}, page_content='35\\n3.1\\nStep-back prompting\\nrange of information, making it easier for the model to identify relevant facts without\\ngetting bogged down by the specifics.\\n The authors used an LLM for the query rewriting task, as shown in figure 3.3.\\nLLMs are an excellent fit for query-rewriting tasks as they excel at natural language\\ncomprehension and generation. You don’t have to train or finetune a new model for\\neach task. Instead, you can provide task instructions in the input prompt.\\n The authors of the step-back prompting paper used the system prompt in the fol-\\nlowing listing to instruct the LLM on how to rewrite the input query.\\nstepback_system_message = f\"\"\"    \\nYou are an expert at world knowledge. Your task is to step back\\nand paraphrase a question to a more generic step-back question, which\\nis easier to answer. Here are a few examples\\n        \\n\"input\": \"Could the members of The Police perform lawful arrests?\"\\n\"output\": \"what can the members of The Police do?\"\\n\"input\": \"Jan Sindel’s was born in what country?\"\\n\"output\": \"what is Jan Sindel’s personal history?\"\\n\"\"\"\\nThe system prompt in listing 3.1 begins by giving the LLM a simple instruction to\\nrewrite a user’s question into a more generic, step-back version. On its own, this kind\\nof instruction is known as zero-shot prompting, which relies solely on the LLM’s general\\ncapabilities and understanding of the task, without providing any examples. However,\\nListing 3.1\\nSystem prompt of an LLM for generating step-back questions\\nOriginal question\\nRewritten\\nstep-back question\\nLLM with\\nstep-back prompt\\nWhich team did\\nThierry Audel play\\nfor from 2007 to 2008?\\nWhich teams did\\nThierry Audel play\\nfor in his career?\\nProcess with\\nLLM\\nStep-back output\\nFigure 3.3\\nRewriting queries using the step-back approach with an LLM\\nQuery \\nrewriting \\ninstructions\\nFew-shot \\nexamples'), Document(metadata={'author': 'Tomaž Bratanic and Oskar Hane', 'creationDate': 'D:20250707132929Z', 'creationdate': '2025-07-07T13:29:29+00:00', 'creator': 'FrameMaker 16.0.1', 'file_path': './data_stored/Essential-GraphRAG.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20250714121407+02'00'\", 'moddate': '2025-07-14T12:14:07+02:00', 'page': 56, 'pk': 461791213351337986, 'producer': 'Acrobat Distiller 20.0 (Windows)', 'source': './data_stored/Essential-GraphRAG.pdf', 'subject': '', 'title': 'Essential GraphRAG', 'total_pages': 179, 'trapped': ''}, page_content='The parent document retriever strategy involves dividing a large document into\\nsmaller sections, calculating embeddings for each section rather than the whole docu-\\nment, and using these embeddings to match user queries more accurately, ultimately\\nListing 3.2\\nFunction to generate a step-back question\\nListing 3.3\\nExecuting the step-back prompt function\\nExercise 3.1\\nTo explore the step-back prompt generation’s effectiveness, try applying it to various\\nquestions and observe how it broadens the context. You can also change the system\\nprompt to observe how it affects the output.'), Document(metadata={'author': 'Tomaž Bratanic and Oskar Hane', 'creationDate': 'D:20250707132929Z', 'creationdate': '2025-07-07T13:29:29+00:00', 'creator': 'FrameMaker 16.0.1', 'file_path': './data_stored/Essential-GraphRAG.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20250714121407+02'00'\", 'moddate': '2025-07-14T12:14:07+02:00', 'page': 56, 'pk': 461791213351337985, 'producer': 'Acrobat Distiller 20.0 (Windows)', 'source': './data_stored/Essential-GraphRAG.pdf', 'subject': '', 'title': 'Essential GraphRAG', 'total_pages': 179, 'trapped': ''}, page_content='36\\nCHAPTER 3\\nAdvanced vector retrieval strategies\\nto guide the model more effectively and ensure consistent results, the authors chose\\nto expand the prompt with several examples of the desired paraphrasing behavior.\\nThis technique is called few-shot prompting, where a small number of examples (typi-\\ncally two to five) are included in the prompt to illustrate the task. Few-shot prompting\\nhelps the LLM better understand the expected transformation by anchoring it in con-\\ncrete instances, improving the quality and reliability of the output.\\n To achieve the query rewriting, all you need to do is send the system prompt found\\nin listing 3.1 along with the user’s question to an LLM. The specific function for this\\ntask is outlined in the next listing.\\ndef generate_stepback(question: str):\\n    user_message = f\"\"\"{question}\"\"\"\\n    step_back_question = chat(\\n        messages=[\\n            {\"role\": \"system\", \"content\": stepback_system_message},\\n            {\"role\": \"user\", \"content\": user_message},\\n        ]\\n    )\\n    return step_back_question\\nYou can now test the step-back prompt generation by executing the code shown next.\\nquestion = \"Which team did Thierry Audel play for from 2007 to 2008?\"\\nstep_back_question = generate_stepback(question)\\nprint(f\"Stepback results: {step_back_question}\")\\n# Stepback results: What is the career history of Thierry Audel?\\nThe results in listing 3.3 demonstrate a successful execution of the step-back prompt\\ngeneration function. By transforming the specific query about Thierry Audel’s team\\nfrom 2007 to 2008 into a broader question regarding his entire career history, the\\nfunction effectively broadens the context and should increase the retrieval accuracy\\nand recall.\\n3.2\\nParent document retriever\\nThe parent document retriever strategy involves dividing a large document into\\nsmaller sections, calculating embeddings for each section rather than the whole docu-\\nment, and using these embeddings to match user queries more accurately, ultimately'), Document(metadata={'author': 'Ian Robinson', 'creationDate': 'D:20150430130949Z', 'creationdate': '2015-04-30T13:09:49+00:00', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'file_path': './data_stored/OReilly_Graph_Databases.pdf', 'format': 'PDF 1.6', 'keywords': '', 'modDate': \"D:20150430091410-04'00'\", 'moddate': '2015-04-30T09:14:10-04:00', 'page': 55, 'pk': 461791213351337987, 'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'source': './data_stored/OReilly_Graph_Databases.pdf', 'subject': '', 'title': 'Graph Databases', 'total_pages': 238, 'trapped': ''}, page_content='Graph Modeling in a Systems Management Domain\\nWe’ve seen how relational modeling and its attendant implementation activities take\\nus down a path that divorces an application’s underlying storage model from the con‐\\nceptual worldview of its stakeholders. Relational databases—with their rigid schemas\\nand complex modeling characteristics—are not an especially good tool for supporting\\nrapid change. What we need is a model that is closely aligned with the domain, but\\nthat doesn’t sacrifice performance, and that supports evolution while maintaining the\\nintegrity of the data as it undergoes rapid change and growth. That model is the\\ngraph model. How, then, does this process differ when realized with a graph data\\nmodel?\\nIn the early stages of analysis, the work required of us is similar to the relational\\napproach: using lo-fi methods, such as whiteboard sketches, we describe and agree\\nupon the domain. After that, however, the methodologies diverge. Instead of trans‐\\nforming a domain model’s graph-like representation into tables, we enrich it, with the\\naim of producing an accurate representation of the parts of the domain relevant to\\nour application goals. That is, for each entity in our domain, we ensure that we’ve\\ncaptured its relevant roles as labels, its attributes as properties, and its connections to\\nneighboring entities as relationships.\\nRemember, the domain model is not a transparent, context-free\\nwindow onto reality: rather, it is a purposeful abstraction of those\\naspects of our domain relevant to our application goals. There’s\\nalways some motivation for creating a model. By enriching our\\nfirst-cut domain graph with additional properties and relation‐\\nships, we effectively produce a graph model attuned to our applica‐\\ntion’s data needs; that is, we provide for answering the kinds of\\nquestions our application will ask of its data.\\nHelpfully, domain modeling is completely isomorphic to graph modeling. By ensur‐\\ning the correctness of the domain model, we’re implicitly improving the graph model,\\nbecause in a graph database what you sketch on the whiteboard is typically what you\\nstore in the database.\\nIn graph terms, what we’re doing is ensuring that each node has the appropriate role-')]\n"
     ]
    }
   ],
   "source": [
    "# testing GraphRAG with vector + graph retrieval\n",
    "\n",
    "# Example input data\n",
    "# question = \"what is Step-back prompting\"\n",
    "\n",
    "# docs = retriever.invoke(question)\n",
    "\n",
    "# print(len(docs))\n",
    "# print(docs,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-back prompting rewrites a question into a more generic one. This makes it easier for the model to identify relevant facts. It uses an LLM to transform a specific query into a broader question.\n"
     ]
    }
   ],
   "source": [
    "# vector_context = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "\n",
    "# print(vector_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N52', status_description='warn: property key does not exist. The property `abstract` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=4, column=19, offset=104>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 104, 'line': 4, 'column': 19}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: \"cypher\\nMATCH (p:Paper)\\nWHERE TOLOWER(p.title) CONTAINS TOLOWER('Step-back prompting')\\nRETURN p.title, p.abstract, p.url\\n\"\n",
      "Received notification from DBMS server: <GqlStatusObject gql_status='01N52', status_description='warn: property key does not exist. The property `url` does not exist in database `neo4j`. Verify that the spelling is correct.', position=<SummaryInputPosition line=4, column=31, offset=116>, raw_classification='UNRECOGNIZED', classification=<NotificationClassification.UNRECOGNIZED: 'UNRECOGNIZED'>, raw_severity='WARNING', severity=<NotificationSeverity.WARNING: 'WARNING'>, diagnostic_record={'_classification': 'UNRECOGNIZED', '_severity': 'WARNING', '_position': {'offset': 116, 'line': 4, 'column': 31}, 'OPERATION': '', 'OPERATION_CODE': '0', 'CURRENT_SCHEMA': '/'}> for query: \"cypher\\nMATCH (p:Paper)\\nWHERE TOLOWER(p.title) CONTAINS TOLOWER('Step-back prompting')\\nRETURN p.title, p.abstract, p.url\\n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (p:Paper)\n",
      "WHERE TOLOWER(p.title) CONTAINS TOLOWER('Step-back prompting')\n",
      "RETURN p.title, p.abstract, p.url\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'what is Step-back prompting', 'result': [], 'intermediate_steps': [{'query': \"cypher\\nMATCH (p:Paper)\\nWHERE TOLOWER(p.title) CONTAINS TOLOWER('Step-back prompting')\\nRETURN p.title, p.abstract, p.url\\n\"}]}\n"
     ]
    }
   ],
   "source": [
    "# graph_context = graph_rag_chain.invoke({\"query\": question})\n",
    "\n",
    "# print(graph_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-back prompting rewrites a question into a more generic one. This helps the model identify relevant facts. It uses an LLM to transform a specific query into a broader question.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# answer = composite_chain.invoke(\n",
    "#     {\"question\": question, \"context\": vector_context, \"graph_context\": graph_context}\n",
    "# )\n",
    "\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preamble or explanation.\n",
    "    \n",
    "    Here are the facts:\n",
    "    {documents} \n",
    "\n",
    "    Here is the answer: \n",
    "    {generation}\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "\"\"\"testing hallucination grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'no'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "     \n",
    "    Here is the answer:\n",
    "    {generation} \n",
    "\n",
    "    Here is the question: {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "\"\"\" testing answer grader\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_672463/3223396883.py:28: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'vectorstore'}\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert at routing a user question to the most appropriate data source. \n",
    "    You have three options:\n",
    "    1. 'vectorstore': Use for questions about explanations of topics.\n",
    "    2. 'graphrag': Use for questions that involve relationships between concepts.\n",
    "    3. 'web_search': Use for all other questions or when current information is needed.\n",
    "\n",
    "    You do not need to be stringent with the keywords in the question related to these topics. \n",
    "    Choose the most appropriate option based on the nature of the question.\n",
    "\n",
    "    Return a JSON with a single key 'datasource' and no preamble or explanation. \n",
    "    The value should be one of: 'vectorstore', 'graphrag', or 'web_search'.\n",
    "    \n",
    "    Question to route: \n",
    "    {question}\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = prompt | llm | JsonOutputParser()\n",
    "\n",
    "\"\"\"testing Router\n",
    "question = \"what is Step-back prompting\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(question_router.invoke({\"question\": question}))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_672463/730760015.py:5: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  web_search_tool = TavilySearchResults(k=3)\n"
     ]
    }
   ],
   "source": [
    "### Search\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement these as a control flow in LangGraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7a3296927f90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "### State\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "        graph_context: results from graph search\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "    graph_context: str\n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "### Nodes\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents and graph context\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    graph_context = state.get(\"graph_context\", \"\")\n",
    "\n",
    "    # Composite RAG generation\n",
    "    generation = composite_chain.invoke(\n",
    "        {\"question\": question, \"context\": documents, \"graph_context\": graph_context}\n",
    "    )\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"generation\": generation,\n",
    "        \"graph_context\": graph_context,\n",
    "    }\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])  # Use get() with a default empty list\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "### Conditional edge\n",
    "def route_question(state):\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    print(source)\n",
    "    print(source[\"datasource\"])\n",
    "\n",
    "    if source[\"datasource\"] == \"graphrag\":\n",
    "        print(\"---TRYING GRAPH SEARCH---\")\n",
    "        graph_result = graph_search({\"question\": question})\n",
    "        if graph_result[\"graph_context\"] != \"No results found in the graph database.\":\n",
    "            return \"graphrag\"\n",
    "        else:\n",
    "            print(\"---NO RESULTS IN GRAPH, FALLING BACK TO VECTORSTORE---\")\n",
    "            return \"retrieve\"\n",
    "    elif source[\"datasource\"] == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO VECTORSTORE RAG---\")\n",
    "        return \"retrieve\"\n",
    "    elif source[\"datasource\"] == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def graph_search(state):\n",
    "    \"\"\"\n",
    "    Perform GraphRAG search using Neo4j\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updated state with graph search results\n",
    "    \"\"\"\n",
    "    print(\"---GRAPH SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Use the graph_rag_chain to perform the search\n",
    "    result = graph_rag_chain.invoke({\"query\": question})\n",
    "\n",
    "    # Extract the relevant information from the result\n",
    "    # Adjust this based on what graph_rag_chain returns\n",
    "    graph_context = result.get(\"result\", \"\")\n",
    "\n",
    "    # You might want to combine this with existing documents or keep it separate\n",
    "    return {\"graph_context\": graph_context, \"question\": question}\n",
    "\n",
    "\n",
    "### Conditional edge\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = grade = score.get(\"score\", \"\").lower()\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7a3296927f90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"graphrag\", graph_search)\n",
    "\n",
    "# Set conditional entry point\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"graphrag\": \"graphrag\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_edge(\"graphrag\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "what is Step-back prompting?\n",
      "{'datasource': 'vectorstore'}\n",
      "vectorstore\n",
      "---ROUTE QUESTION TO VECTORSTORE RAG---\n",
      "---RETRIEVE---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\n",
      "'Finished running: grade_documents:'\n",
      "---WEB SEARCH---\n",
      "'Finished running: websearch:'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "'Finished running: generate:'\n",
      "('Step-back prompting is a technique that encourages a language model to '\n",
      " 'consider abstractions and first principles before answering a question. It '\n",
      " 'involves two steps: abstraction, where the model identifies higher-level '\n",
      " 'concepts, and reasoning, where it uses those concepts to answer the '\n",
      " 'question. This approach can improve the accuracy of complex reasoning tasks.')\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"what is Step-back prompting?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile\n",
    "# app = workflow.compile()\n",
    "\n",
    "# # Test\n",
    "# from pprint import pprint\n",
    "\n",
    "# inputs = {\"question\": \"Did Emmanuel Macron visit Germany recently?\"}\n",
    "# for output in app.stream(inputs):\n",
    "#     for key, value in output.items():\n",
    "#         pprint(f\"Finished running: {key}:\")\n",
    "# pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test\n",
    "# from pprint import pprint\n",
    "\n",
    "# inputs = {\"question\": \"Which paper talk about large language models?\"}\n",
    "# for output in app.stream(inputs):\n",
    "#     for key, value in output.items():\n",
    "#         pprint(f\"Finished running: {key}:\")\n",
    "# pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
