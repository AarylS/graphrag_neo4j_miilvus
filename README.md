
# Graph rag with local pdfs

implementing a graphrag using neo4j and milvus on locally stored pdfs.

this code follows - https://github.com/milvus-io/bootcamp/blob/master/bootcamp/RAG/advanced_rag/langgraph-graphrag-agent-local.ipynb

it is changed to run with google-gemini

graphrag
    this is a rag system with a graph to store the relation between possible entities. this provides better/improved context to the llm for grounding. rag are good at finding relevent texts and graphrag are good at finding relevent connections

    in graphrag, entities and their relations are extracted from chunks and store in a graph database like neo4j. these are store as nodes and edges.


## techstack
langchain
langgraph
google gemini - llm models
huggingface - embedding model
neo4j - graphdb
milvus-lite - vectordb
tavily - websearch

## pre requisites

    install uv.

    install docker desktop


## installation

1. Clone the repository  
   -bash-
   git clone https://github.com/AarylS/graphrag_neo4j_miilvus.git
   cd graphrag_neo4j_miilvus

2. create virtual environment
    -bash-
    uv venv .venv
    source .venv/bin/activate

3. install dependencies
    -bash-
    uv pip install -r requirements.txt

4. set docker container
    -bash-
    docker compose up -d


# Environment Variables

this file stores the api keys.

important: create a .env file in the root directory of the project
-code-
NEO4J_URI="bolt://localhost:7687"
NEO4J_USERNAME="neo4j"
NEO4J_PASSWORD="enter your neo4j password" 

TAVILY_API_KEY=""
GOOGLE_API_KEY=""


# Usage

1. in the ./data_stored folder in project root directoy 
    remove the demo pdfs and store the required pdf docs here.
    these docs will be used in graphrag


2. open jupyter notebook - graph_rag_att_5.ipynb

3. on the last cell in the notebook. 
    search for the line - inputs = {"question": } -
    to this line add the following after "question": - "add your question"
    eg. inputs = {"question": "add your question"}

4. click on run all.

note: you could change the prompts on langchain pipelines mented below to include details on your data sources.
    this improves the accuracy of the results of your graphrag.


## project structure

One jupyter notebook. contains full project.
the flow of code is broken down below.

- setting up models 
    here the llm models used in project are initiated.
    1. embedding model: to create vector embedding for vectordb
    2. cypher generation graph model: to generate cypher queries  
    3. answering model: general purpose llm for the rest of the tasks/answers generated.

- data loading and processing(chunking)
    pdfs from data_stored folder are accessed and its pages loaded.
    the loaded pages are broken down into smaller chunks(divided into smaller potions)

- creating vector storage and retriever
    here the chunks are embedded and stored into a vector database using milvus.
    a retriever is initialized to access(pull context from) this database.

- creating the graph
    a graph is created using the information obtained from the chunks. informations are extracted and stored in a graph using the graph llm(for extracting info and cypher query generation) and neo4j(for storing graph in graph database).

- langchain pipelines for:
    1. retrieval grader
        this checks the info/context retrieved from the vector store and return a json with yes or no.

    2. answer generator
        this genearates the answer with context from vector store.

    3. graph generator
        here the graph context is retrieved from the graph database by generating the cypher query based on the question. it then generates the answer based on the context and question.

    4. graphrag generator
        here the answer to the question is generated using the context from vector and graph stored.

    5. hallucination grader
        compares context retrieved to the answer generated by answer generator. this checks if answer is grounded. this generates yes(answer grounded) or no(answer not grounded).

    6. answer grader: 
        checks if the answer generated by answer generator resolves the question asked. this generates yes or no. 

    7. router: 
        decides/selects which process to use, options:vectorstore, graphrag, websearch, based on the question.
        this generates a json - {'datasource': 'option selected'}

    8. websearch using tavily
        searches for context on the web and return top 3 results

- langgraph state and nodes
    state: 
        this is used to access/store data by the nodes.
    nodes: 
        these are function that perform tasks with the above pipelines.  

- langgraph workflow 
    here the nodes are defined and the workflow is set. this workflow defines the flow of execution of the graphrag.

    check workflow diagram for the execution flow.
    [add diagram below]

- complie and run workflow
    here the langgraph workflow is compiled and run with a question.
    add your question in this part of the code.



## END



