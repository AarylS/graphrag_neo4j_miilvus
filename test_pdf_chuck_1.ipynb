{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57867a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf015886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader # <-- The key change is this import\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- PDF LOADING AND PROCESSING using PyMuPDF ---\n",
    "\n",
    "# 1. Define the path to your folder of PDFs\n",
    "pdf_folder_path = \"./data_stored/\"\n",
    "print(f\"Loading PDFs from: {pdf_folder_path}\")\n",
    "\n",
    "# 2. List all the PDF files in the folder\n",
    "pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith(\".pdf\")]\n",
    "\n",
    "# 3. Load all the documents from the PDF files\n",
    "all_documents = []\n",
    "for pdf_file in pdf_files:\n",
    "    file_path = os.path.join(pdf_folder_path, pdf_file)\n",
    "    print(f\"  - Loading document: {pdf_file}\")\n",
    "    # Use PyMuPDFLoader instead of PyPDFLoader\n",
    "    loader = PyMuPDFLoader(file_path)\n",
    "    # The loader splits the PDF into pages, each page is a Document\n",
    "    pages = loader.load()\n",
    "\n",
    "    start_index = 50  # Keep pages from page 22 onwards\n",
    "    end_index = 52   # Keep pages before page 172\n",
    "\n",
    "    cleaned_pages = [\n",
    "        page for page in pages \n",
    "        if start_index <= page.metadata.get('page', 0) < end_index\n",
    "    ]\n",
    "\n",
    "    all_documents.extend(pages)\n",
    "\n",
    "print(f\"Loaded a total of {len(all_documents)} pages from {len(pdf_files)} PDF files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Split the loaded documents into smaller chunks (This part is unchanged)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"Split the documents into {len(doc_splits)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da29d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for r in doc_splits:\n",
    "#     print(r)  # Print the first 200 characters of each chunk\n",
    "#     print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279edc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "\n",
    "repo_id = \"meta-llama/Llama-3.1-8B-Instruct\"     # answer llm\n",
    "graph_llm_name = \"gemini-2.5-flash\" \n",
    "embed_model_name = \"all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb90045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# model_name=\"all-MiniLM-L6-v2\"\n",
    "# model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=embed_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10855471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus import Milvus\n",
    "\n",
    "\n",
    "\n",
    "# Add to Milvus\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag_milvus_test_3\",\n",
    "    embedding=embedding_model,\n",
    "    connection_args={\"uri\": \"./milvus_ingest_test_3.db\"},\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c58d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "graph_llm = ChatGoogleGenerativeAI(model=graph_llm_name, temperature=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GraphRAG Setup\n",
    "# from langchain_community.graphs import Neo4jGraph\n",
    "# from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "# from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_ollama import ChatOllama\n",
    "\n",
    "# graph = Neo4jGraph()\n",
    "\n",
    "\n",
    "# graph_transformer = LLMGraphTransformer(\n",
    "#     llm=graph_llm,\n",
    "# )\n",
    "# print(\"1\")\n",
    "\n",
    "# graph_documents = graph_transformer.convert_to_graph_documents(doc_splits)\n",
    "# print(\"2\")\n",
    "# graph.add_graph_documents(graph_documents)\n",
    "# print(\"3\")\n",
    "# print(f\"Graph documents: {len(graph_documents)}\")\n",
    "# print(f\"Nodes from 1st graph doc:{graph_documents[0].nodes}\")\n",
    "# print(f\"Relationships from 1st graph doc:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44106710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # After converting to graph documents\n",
    "# for i, doc in enumerate(graph_documents):\n",
    "#     print(f\"Document {i}:\")\n",
    "#     print(f\"  Nodes: {doc.nodes}\")\n",
    "#     print(f\"  Relationships: {doc.relationships}\")\n",
    "#     print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa66590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7effa314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphRAG Setup\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "import time\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "\n",
    "\n",
    "graph_transformer = LLMGraphTransformer(\n",
    "    llm=graph_llm,\n",
    ")\n",
    "print(\"1\")\n",
    "\n",
    "graph_documents = []\n",
    "for doc in doc_splits:\n",
    "    stime = time.time()\n",
    "    # Process one document at a time\n",
    "    graph_doc = graph_transformer.convert_to_graph_documents([doc])\n",
    "    graph_documents.extend(graph_doc)\n",
    "    print(f\"end of chunk, time taken: {time.time() - stime}\")\n",
    "    \n",
    "print(\"2\")\n",
    "graph.add_graph_documents(graph_documents)\n",
    "print(\"3\")\n",
    "print(f\"Graph documents: {len(graph_documents)}\")\n",
    "print(f\"Nodes from 1st graph doc:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships from 1st graph doc:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee75486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # After converting to graph documents\n",
    "# for i, doc in enumerate(graph_documents):\n",
    "#     print(f\"Document {i}:\")\n",
    "#     print(f\"  Nodes: {doc.nodes}\")\n",
    "#     print(f\"  Relationships: {doc.relationships}\")\n",
    "#     print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39dcca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\", temperature=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c343db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n",
    "    \n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "     \n",
    "    Here is the retrieved document: \n",
    "    {document}\n",
    "    \n",
    "    Here is the user question: \n",
    "    {question}\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     template=\"\"\"You are a grader assessing relevance\n",
    "# of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
    "# grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
    "\n",
    "# Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "# Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "\n",
    "# **IMPORTANT:** Your response MUST be only the JSON object itself, without any surrounding text or markdown.\n",
    "\n",
    "# Here is the retrieved document:\n",
    "# {document}\n",
    "\n",
    "# Here is the user question:\n",
    "# {question}\n",
    "# \"\"\",\n",
    "#     input_variables=[\"question\", \"document\"],\n",
    "# )\n",
    "\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"What is a graph?\"\n",
    "docs = retriever.invoke(question)\n",
    "print(\"num of docs\", len(docs))\n",
    "doc_txt = docs[1].page_content\n",
    "print(doc_txt)\n",
    "print(\n",
    "    f'Is our answer relevant to the question asked: {retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    Also only answer the question based on the context provided. If you don't know the answer, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "question = \"What are graphrag?\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d694f022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
